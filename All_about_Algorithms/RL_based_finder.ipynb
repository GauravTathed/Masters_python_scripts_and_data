{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da790e-e3d7-41bb-a5a0-1a3f05c89aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Beta\n",
    "from scipy.linalg import expm\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions (as provided)\n",
    "# ---------------------------\n",
    "\n",
    "def coupling_operator_with_phase(i, j, dim, phi):\n",
    "    op = np.zeros((dim, dim), dtype=complex)\n",
    "    op[i, j] = np.exp(+1j * phi)\n",
    "    op[j, i] = np.exp(-1j * phi)\n",
    "    return op\n",
    "\n",
    "def pulse_duration_for_fraction(f, Omega):\n",
    "    theta = np.pi * np.array(f)\n",
    "    return theta / Omega if Omega != 0 else 0.0\n",
    "\n",
    "def unitary(couplings, rabi_freqs, fractions, fixed_phase_flags, dim):\n",
    "    U_seq = np.eye(dim, dtype=complex)\n",
    "    for (levels, Omega, frac, fix_pflag) in zip(couplings, rabi_freqs, fractions, fixed_phase_flags):\n",
    "        i, j = levels\n",
    "        phi_fixed = fix_pflag * np.pi\n",
    "        total_phase = phi_fixed\n",
    "        H_op = coupling_operator_with_phase(i, j, dim, total_phase)\n",
    "        H_coupling = 0.5 * Omega * H_op\n",
    "        t_pulse = pulse_duration_for_fraction(frac, Omega)\n",
    "        U_pulse = expm(-1j * H_coupling * t_pulse)\n",
    "        U_seq = U_pulse @ U_seq\n",
    "    return U_seq\n",
    "\n",
    "def fix_couplings_and_phases(couplings, fixed_phase_flags):\n",
    "    new_couplings = []\n",
    "    new_fixed_phase_flags = []\n",
    "    for (cpl, phase_flag) in zip(couplings, fixed_phase_flags):\n",
    "        i, j = cpl\n",
    "        if i != 0 and j == 0:\n",
    "            cpl_fixed = (0, i)\n",
    "            phase_flag_fixed = phase_flag + 1.0\n",
    "        else:\n",
    "            cpl_fixed = cpl\n",
    "            phase_flag_fixed = phase_flag\n",
    "        new_couplings.append(cpl_fixed)\n",
    "        new_fixed_phase_flags.append(phase_flag_fixed)\n",
    "    return new_couplings, new_fixed_phase_flags\n",
    "\n",
    "def compute_fidelity(U_target, U_pred, dim):\n",
    "    # Fidelity measure using the normalized absolute trace overlap.\n",
    "    fid = np.abs(np.trace(np.conjugate(U_target.T) @ U_pred)) / dim\n",
    "    return fid\n",
    "\n",
    "# ---------------------------\n",
    "# Problem Setup Parameters\n",
    "# ---------------------------\n",
    "\n",
    "dim = 5                  # Hilbert space dimension\n",
    "sequence_length = 7      # Number of pulses in the sequence\n",
    "max_fraction = 1.5       # Maximum value for the 'fraction' parameter\n",
    "# For the RL agent, we fix rabi frequencies to 1\n",
    "rabi_freqs = [1] * sequence_length\n",
    "\n",
    "# We define a list of allowed couplings (discrete choices) for the policy.\n",
    "allowed_couplings = [(0,1), (0,2), (0,3), (0,4), (1,0), (2,0), (3,0), (4,0)]\n",
    "num_allowed_couplings = len(allowed_couplings)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Generation Functions\n",
    "# ---------------------------\n",
    "def generate_random_sequence():\n",
    "    \"\"\"Generates a random pulse sequence using uniform sampling.\n",
    "       Returns:\n",
    "          couplings: list of tuples (each chosen from allowed_couplings)\n",
    "          fractions: list of continuous parameters in [0, max_fraction]\n",
    "          fixed_phase_flags: list of binary values (0 or 1)\n",
    "    \"\"\"\n",
    "    couplings = []\n",
    "    fractions = []\n",
    "    fixed_phase_flags = []\n",
    "    for _ in range(sequence_length):\n",
    "        # Choose a random allowed coupling (an index into allowed_couplings)\n",
    "        idx = np.random.randint(0, num_allowed_couplings)\n",
    "        couplings.append(allowed_couplings[idx])\n",
    "        # Sample a random fraction uniformly\n",
    "        frac = np.random.uniform(0.0, max_fraction)\n",
    "        fractions.append(frac)\n",
    "        # Sample a binary fixed-phase flag\n",
    "        fix_flag = np.random.randint(0, 2)\n",
    "        fixed_phase_flags.append(fix_flag)\n",
    "        \n",
    "    # Apply the fix function (if applicable)\n",
    "    couplings, fixed_phase_flags = fix_couplings_and_phases(couplings, fixed_phase_flags)\n",
    "    return couplings, fractions, fixed_phase_flags\n",
    "\n",
    "def generate_random_target_unitary():\n",
    "    \"\"\"Generates a target unitary by sampling a random pulse sequence.\n",
    "       Returns:\n",
    "          U_target (np.ndarray): the resulting unitary matrix.\n",
    "          parameters: the pulse parameters used (for reference).\n",
    "    \"\"\"\n",
    "    couplings, fractions, fixed_phase_flags = generate_random_sequence()\n",
    "    U_target = unitary(couplings, rabi_freqs, fractions, fixed_phase_flags, dim)\n",
    "    return U_target, (couplings, fractions, fixed_phase_flags)\n",
    "\n",
    "# ---------------------------\n",
    "# Policy Network Definition\n",
    "# ---------------------------\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sequence_length, num_couplings):\n",
    "        \"\"\"\n",
    "        input_dim: dimension of flattened target unitary (real and imaginary parts concatenated)\n",
    "        hidden_dim: hidden layer size\n",
    "        sequence_length: number of pulses (time steps)\n",
    "        num_couplings: number of allowed coupling choices.\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_couplings = num_couplings\n",
    "        \n",
    "        # A simple MLP that outputs parameters for each step.\n",
    "        # Total output per time step: coupling logits (num_couplings) + fixed-phase logits (2) + fraction parameters (2 for Beta: raw_alpha, raw_beta)\n",
    "        self.output_per_step = num_couplings + 2 + 2  \n",
    "        total_output_dim = sequence_length * self.output_per_step\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, total_output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        # For numerical stability in Beta parameters\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: tensor of shape (batch_size, input_dim)\n",
    "        Returns for each sample in batch: a list of actions per step and the sum of their log probabilities.\n",
    "        Each action is a tuple: (coupling_index, fraction, fixed_phase_flag)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        # Reshape output: (batch_size, sequence_length, output_per_step)\n",
    "        out = out.view(batch_size, self.sequence_length, self.output_per_step)\n",
    "        \n",
    "        actions_batch = []  # list with length batch_size; each element is a list (length sequence_length) of actions\n",
    "        log_probs = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            actions = []\n",
    "            log_prob_sum = 0\n",
    "            # Process each time step\n",
    "            for t in range(self.sequence_length):\n",
    "                step_out = out[b, t]\n",
    "                # Split the outputs for the step:\n",
    "                coupling_logits = step_out[0:self.num_couplings]            # shape: (num_couplings,)\n",
    "                fixed_logits    = step_out[self.num_couplings:self.num_couplings+2]  # binary decision logits\n",
    "                fraction_raw    = step_out[self.num_couplings+2:self.num_couplings+4]  # for Beta parameters\n",
    "                \n",
    "                # Sample coupling from a categorical distribution.\n",
    "                coupling_dist = Categorical(logits=coupling_logits)\n",
    "                coupling_index = coupling_dist.sample()\n",
    "                log_prob_sum += coupling_dist.log_prob(coupling_index)\n",
    "                \n",
    "                # Sample fixed-phase flag from binary categorical distribution.\n",
    "                fixed_dist = Categorical(logits=fixed_logits)\n",
    "                fixed_phase_flag = fixed_dist.sample()\n",
    "                log_prob_sum += fixed_dist.log_prob(fixed_phase_flag)\n",
    "                \n",
    "                # For fraction, we use a Beta distribution. To ensure positive concentration parameters, use softplus.\n",
    "                # Add 1 to avoid too-small parameters.\n",
    "                alpha = self.softplus(fraction_raw[0]) + 1.0\n",
    "                beta_param = self.softplus(fraction_raw[1]) + 1.0\n",
    "                fraction_dist = Beta(alpha, beta_param)\n",
    "                fraction_sample = fraction_dist.rsample()  # using rsample for a reparameterized sample if needed\n",
    "                # Scale the sample to our desired range [0, max_fraction]\n",
    "                fraction = fraction_sample * max_fraction\n",
    "                log_prob_sum += fraction_dist.log_prob(fraction_sample)\n",
    "                \n",
    "                # Append the actions (convert tensor values to Python numbers)\n",
    "                actions.append({\n",
    "                    'coupling': int(coupling_index.item()),\n",
    "                    'fraction': fraction.item(),\n",
    "                    'fixed_phase_flag': int(fixed_phase_flag.item())\n",
    "                })\n",
    "            actions_batch.append(actions)\n",
    "            log_probs.append(log_prob_sum)\n",
    "        \n",
    "        # Return tensors: here we assume batch_size=1 for simplicity; otherwise further batching is needed.\n",
    "        return actions_batch, torch.stack(log_probs)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Convert Target Unitary to Network Input\n",
    "# ---------------------------\n",
    "def target_unitary_to_tensor(U):\n",
    "    \"\"\"\n",
    "    Convert U (a complex matrix) into a real tensor that concatenates the flattened real and imaginary parts.\n",
    "    \"\"\"\n",
    "    U_real = np.real(U).flatten()\n",
    "    U_imag = np.imag(U).flatten()\n",
    "    U_concat = np.concatenate([U_real, U_imag])\n",
    "    return torch.tensor(U_concat, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop (REINFORCE)\n",
    "# ---------------------------\n",
    "def train_policy(num_episodes=1000, lr=1e-3, print_every=100):\n",
    "    # The input dimension is: dim*dim*2 (real & imag parts)\n",
    "    input_dim = dim * dim * 2  \n",
    "    hidden_dim = 128\n",
    "    policy_net = PolicyNetwork(input_dim, hidden_dim, sequence_length, num_allowed_couplings)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    # For a simple baseline, we use a running average reward.\n",
    "    baseline = None\n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate a target unitary (the \"desired\" evolution)\n",
    "        U_target, true_params = generate_random_target_unitary()\n",
    "        # Convert target unitary to network input format.\n",
    "        target_tensor = target_unitary_to_tensor(U_target).unsqueeze(0)  # shape: (1, input_dim)\n",
    "        \n",
    "        # Forward pass: sample a candidate pulse sequence.\n",
    "        actions_batch, log_prob = policy_net(target_tensor)\n",
    "        # We assume a single sample (batch_size=1).\n",
    "        actions = actions_batch[0]\n",
    "        \n",
    "        # Build candidate sequence parameters from actions:\n",
    "        # For couplings, use the selected coupling from allowed_couplings.\n",
    "        candidate_couplings = []\n",
    "        candidate_fractions = []\n",
    "        candidate_fixed_flags = []\n",
    "        for action in actions:\n",
    "            # Get the coupling pair from the allowed list using the sampled index.\n",
    "            candidate_couplings.append(allowed_couplings[action['coupling']])\n",
    "            candidate_fractions.append(action['fraction'])\n",
    "            candidate_fixed_flags.append(action['fixed_phase_flag'])\n",
    "        \n",
    "        # Optionally, fix the couplings and phases (as in your helper function)\n",
    "        candidate_couplings, candidate_fixed_flags = fix_couplings_and_phases(candidate_couplings, candidate_fixed_flags)\n",
    "        U_pred = unitary(candidate_couplings, rabi_freqs, candidate_fractions, candidate_fixed_flags, dim)\n",
    "        \n",
    "        # Compute reward based on fidelity.\n",
    "        reward = compute_fidelity(U_target, U_pred, dim)\n",
    "        reward_history.append(reward)\n",
    "        \n",
    "        # Update running baseline (simple average)\n",
    "        if baseline is None:\n",
    "            baseline = reward\n",
    "        else:\n",
    "            baseline = 0.99 * baseline + 0.01 * reward\n",
    "        \n",
    "        # REINFORCE loss: we want to maximize reward, so minimize -log_prob * (reward - baseline)\n",
    "        loss = -log_prob * (reward - baseline)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(reward_history[-print_every:])\n",
    "            print(f\"Episode {episode+1}, Loss: {loss.item():.4f}, Reward: {reward:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
    "    \n",
    "    return policy_net\n",
    "\n",
    "# ---------------------------\n",
    "# Testing the Trained Policy\n",
    "# ---------------------------\n",
    "def test_policy(policy_net, num_tests=10):\n",
    "    test_rewards = []\n",
    "    for i in range(num_tests):\n",
    "        U_target, true_params = generate_random_target_unitary()\n",
    "        target_tensor = target_unitary_to_tensor(U_target).unsqueeze(0)\n",
    "        actions_batch, _ = policy_net(target_tensor)\n",
    "        actions = actions_batch[0]\n",
    "        candidate_couplings = []\n",
    "        candidate_fractions = []\n",
    "        candidate_fixed_flags = []\n",
    "        for action in actions:\n",
    "            candidate_couplings.append(allowed_couplings[action['coupling']])\n",
    "            candidate_fractions.append(action['fraction'])\n",
    "            candidate_fixed_flags.append(action['fixed_phase_flag'])\n",
    "        candidate_couplings, candidate_fixed_flags = fix_couplings_and_phases(candidate_couplings, candidate_fixed_flags)\n",
    "        U_pred = unitary(candidate_couplings, rabi_freqs, candidate_fractions, candidate_fixed_flags, dim)\n",
    "        reward = compute_fidelity(U_target, U_pred, dim)\n",
    "        test_rewards.append(reward)\n",
    "        print(f\"Test {i+1}: Fidelity = {reward:.4f}\")\n",
    "    print(f\"Average test fidelity: {np.mean(test_rewards):.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Train the policy network using our RL approach.\n",
    "    policy_net = train_policy(num_episodes=1000, lr=1e-3, print_every=100)\n",
    "    # Evaluate on a test set.\n",
    "    test_policy(policy_net, num_tests=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f6746e-1718-46c2-b4b6-b304b062f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gaurav\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2965f-161a-4d7c-8454-ac213873ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab174196-ef49-4dd7-9e52-f7b717232e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Loss: 1.8815, Reward: 0.4169, Avg Reward: 0.1936\n",
      "Episode 200, Loss: 0.1494, Reward: 0.2496, Avg Reward: 0.2168\n",
      "Episode 300, Loss: -0.5086, Reward: 0.1785, Avg Reward: 0.1994\n",
      "Episode 400, Loss: -0.8907, Reward: 0.0374, Avg Reward: 0.2234\n",
      "Episode 500, Loss: -0.1181, Reward: 0.1697, Avg Reward: 0.1978\n",
      "Episode 600, Loss: 1.0598, Reward: 0.5207, Avg Reward: 0.2122\n",
      "Episode 700, Loss: -0.0964, Reward: 0.1652, Avg Reward: 0.2154\n",
      "Episode 800, Loss: 0.0444, Reward: 0.3038, Avg Reward: 0.2293\n",
      "Episode 900, Loss: -0.5467, Reward: 0.0636, Avg Reward: 0.2190\n",
      "Episode 1000, Loss: 0.1164, Reward: 0.2623, Avg Reward: 0.2331\n",
      "Test 1: Fidelity = 0.0329\n",
      "Test 2: Fidelity = 0.2014\n",
      "Test 3: Fidelity = 0.2818\n",
      "Test 4: Fidelity = 0.2214\n",
      "Test 5: Fidelity = 0.2410\n",
      "Test 6: Fidelity = 0.3204\n",
      "Test 7: Fidelity = 0.2794\n",
      "Test 8: Fidelity = 0.2094\n",
      "Test 9: Fidelity = 0.6352\n",
      "Test 10: Fidelity = 0.2816\n",
      "Average test fidelity: 0.2704\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Beta\n",
    "from scipy.linalg import expm\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions (as provided)\n",
    "# ---------------------------\n",
    "\n",
    "def coupling_operator_with_phase(i, j, dim, phi):\n",
    "    op = np.zeros((dim, dim), dtype=complex)\n",
    "    op[i, j] = np.exp(+1j * phi)\n",
    "    op[j, i] = np.exp(-1j * phi)\n",
    "    return op\n",
    "\n",
    "def pulse_duration_for_fraction(f, Omega):\n",
    "    theta = np.pi * np.array(f)\n",
    "    return theta / Omega if Omega != 0 else 0.0\n",
    "\n",
    "def unitary(couplings, rabi_freqs, fractions, fixed_phase_flags, dim):\n",
    "    U_seq = np.eye(dim, dtype=complex)\n",
    "    for (levels, Omega, frac, fix_pflag) in zip(couplings, rabi_freqs, fractions, fixed_phase_flags):\n",
    "        i, j = levels\n",
    "        phi_fixed = fix_pflag * np.pi\n",
    "        total_phase = phi_fixed\n",
    "        H_op = coupling_operator_with_phase(i, j, dim, total_phase)\n",
    "        H_coupling = 0.5 * Omega * H_op\n",
    "        t_pulse = pulse_duration_for_fraction(frac, Omega)\n",
    "        U_pulse = expm(-1j * H_coupling * t_pulse)\n",
    "        U_seq = U_pulse @ U_seq\n",
    "    return U_seq\n",
    "\n",
    "def fix_couplings_and_phases(couplings, fixed_phase_flags):\n",
    "    new_couplings = []\n",
    "    new_fixed_phase_flags = []\n",
    "    for (cpl, phase_flag) in zip(couplings, fixed_phase_flags):\n",
    "        i, j = cpl\n",
    "        if i != 0 and j == 0:\n",
    "            cpl_fixed = (0, i)\n",
    "            phase_flag_fixed = phase_flag + 1.0\n",
    "        else:\n",
    "            cpl_fixed = cpl\n",
    "            phase_flag_fixed = phase_flag\n",
    "        new_couplings.append(cpl_fixed)\n",
    "        new_fixed_phase_flags.append(phase_flag_fixed)\n",
    "    return new_couplings, new_fixed_phase_flags\n",
    "\n",
    "def compute_fidelity(U_target, U_pred, dim):\n",
    "    # Fidelity measure using the normalized absolute trace overlap.\n",
    "    fid = np.abs(np.trace(np.conjugate(U_target.T) @ U_pred)) / dim\n",
    "    return fid\n",
    "\n",
    "# ---------------------------\n",
    "# Problem Setup Parameters\n",
    "# ---------------------------\n",
    "\n",
    "dim = 5                  # Hilbert space dimension\n",
    "sequence_length = 7      # Number of pulses in the sequence\n",
    "max_fraction = 2    # Maximum value for the 'fraction' parameter\n",
    "# For the RL agent, we fix rabi frequencies to 1 for every pulse.\n",
    "rabi_freqs = [1] * sequence_length\n",
    "\n",
    "# Define a list of allowed couplings (discrete choices) for the policy.\n",
    "allowed_couplings = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "num_allowed_couplings = len(allowed_couplings)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Generation Functions\n",
    "# ---------------------------\n",
    "def generate_random_sequence():\n",
    "    \"\"\"Generates a random pulse sequence using uniform sampling.\n",
    "       Returns:\n",
    "          couplings: list of tuples (each chosen from allowed_couplings)\n",
    "          fractions: list of continuous parameters in [0, max_fraction]\n",
    "          fixed_phase_flags: list of binary values (0 or 1)\n",
    "    \"\"\"\n",
    "    couplings = []\n",
    "    fractions = []\n",
    "    fixed_phase_flags = []\n",
    "    for _ in range(sequence_length):\n",
    "        # Randomly choose a coupling from the allowed list.\n",
    "        idx = np.random.randint(0, num_allowed_couplings)\n",
    "        couplings.append(allowed_couplings[idx])\n",
    "        # Sample a random fraction uniformly.\n",
    "        frac = np.random.uniform(0.0, max_fraction)\n",
    "        fractions.append(frac)\n",
    "        # Sample a binary fixed-phase flag.\n",
    "        fix_flag = np.random.randint(0, 2)\n",
    "        fixed_phase_flags.append(fix_flag)\n",
    "        \n",
    "    # Apply the fix function (if applicable)\n",
    "    couplings, fixed_phase_flags = fix_couplings_and_phases(couplings, fixed_phase_flags)\n",
    "    return couplings, fractions, fixed_phase_flags\n",
    "\n",
    "def generate_random_target_unitary():\n",
    "    \"\"\"Generates a target unitary by sampling a random pulse sequence.\n",
    "       Returns:\n",
    "          U_target (np.ndarray): the resulting unitary matrix.\n",
    "          parameters: the pulse parameters used (for reference).\n",
    "    \"\"\"\n",
    "    couplings, fractions, fixed_phase_flags = generate_random_sequence()\n",
    "    U_target = unitary(couplings, rabi_freqs, fractions, fixed_phase_flags, dim)\n",
    "    return U_target, (couplings, fractions, fixed_phase_flags)\n",
    "\n",
    "# ---------------------------\n",
    "# LSTM-Based Policy Network Definition\n",
    "# ---------------------------\n",
    "class PolicyNetworkLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sequence_length, num_couplings):\n",
    "        \"\"\"\n",
    "        input_dim: Dimension of the flattened target unitary (real and imaginary parts concatenated)\n",
    "        hidden_dim: Hidden layer size for both the encoder and LSTM\n",
    "        sequence_length: Number of pulses (time steps)\n",
    "        num_couplings: Number of allowed coupling choices.\n",
    "        \"\"\"\n",
    "        super(PolicyNetworkLSTM, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_couplings = num_couplings\n",
    "        # Each time step will output:\n",
    "        # - Coupling logits (num_couplings)\n",
    "        # - Fixed-phase logits (2)\n",
    "        # - Fraction parameters (2 for Beta: raw_alpha, raw_beta)\n",
    "        self.output_per_step = num_couplings + 2 + 2\n",
    "\n",
    "        # Encoder: Encode the target unitary into a hidden representation.\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # LSTM decoder: receives a learned start token at each time step.\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        # Learned start token; shape: (1, hidden_dim) which will be repeated.\n",
    "        self.start_token = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        \n",
    "        # Decoder to produce output for each time step from the LSTM hidden state.\n",
    "        self.decoder = nn.Linear(hidden_dim, self.output_per_step)\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, input_dim) representing the target unitary.\n",
    "        Returns for each sample in the batch: a list of actions per time step and the sum of their log probabilities.\n",
    "        Each action is a dictionary with keys: 'coupling', 'fraction', and 'fixed_phase_flag'.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # Encode the target unitary.\n",
    "        encoded = self.relu(self.encoder(x))  # shape: (batch_size, hidden_dim)\n",
    "        # Use the encoded representation as the initial hidden state.\n",
    "        h0 = encoded.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros_like(h0)  # initialize cell state to zeros\n",
    "        \n",
    "        # Prepare LSTM input: we use the same start token at each time step.\n",
    "        # Create tokens with shape: (batch_size, sequence_length, hidden_dim)\n",
    "        tokens = self.start_token.expand(batch_size, self.sequence_length, -1)\n",
    "        \n",
    "        # Generate the sequence from the LSTM.\n",
    "        lstm_out, _ = self.lstm(tokens, (h0, c0))  # shape: (batch_size, sequence_length, hidden_dim)\n",
    "        # Project LSTM outputs to the desired output shape.\n",
    "        decoded = self.decoder(lstm_out)  # shape: (batch_size, sequence_length, output_per_step)\n",
    "        \n",
    "        actions_batch = []  # List to hold actions per sample in the batch.\n",
    "        log_probs = []      # List for the sum of log probabilities for each sample.\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            actions = []\n",
    "            log_prob_sum = 0\n",
    "            # Process each time step.\n",
    "            for t in range(self.sequence_length):\n",
    "                step_out = decoded[b, t]\n",
    "                # Split outputs into parts.\n",
    "                coupling_logits = step_out[0:self.num_couplings]  # Discrete coupling selection.\n",
    "                fixed_logits = step_out[self.num_couplings:self.num_couplings+2]  # Fixed-phase binary decision.\n",
    "                fraction_raw = step_out[self.num_couplings+2:self.num_couplings+4]  # For Beta distribution parameters.\n",
    "                \n",
    "                # Sample from the coupling distribution.\n",
    "                coupling_dist = Categorical(logits=coupling_logits)\n",
    "                coupling_index = coupling_dist.sample()\n",
    "                log_prob_sum = log_prob_sum + coupling_dist.log_prob(coupling_index)\n",
    "                \n",
    "                # Sample from the fixed-phase distribution.\n",
    "                fixed_dist = Categorical(logits=fixed_logits)\n",
    "                fixed_phase_flag = fixed_dist.sample()\n",
    "                log_prob_sum = log_prob_sum + fixed_dist.log_prob(fixed_phase_flag)\n",
    "                \n",
    "                # Sample the fraction from a Beta distribution.\n",
    "                alpha = self.softplus(fraction_raw[0]) + 1.0\n",
    "                beta_param = self.softplus(fraction_raw[1]) + 1.0\n",
    "                fraction_dist = Beta(alpha, beta_param)\n",
    "                fraction_sample = fraction_dist.rsample()  # Use rsample for reparameterization.\n",
    "                # Scale to the desired range [0, max_fraction].\n",
    "                fraction = fraction_sample * max_fraction\n",
    "                log_prob_sum = log_prob_sum + fraction_dist.log_prob(fraction_sample)\n",
    "                \n",
    "                # Append the action.\n",
    "                actions.append({\n",
    "                    'coupling': int(coupling_index.item()),\n",
    "                    'fraction': fraction.item(),\n",
    "                    'fixed_phase_flag': int(fixed_phase_flag.item())\n",
    "                })\n",
    "            actions_batch.append(actions)\n",
    "            log_probs.append(log_prob_sum)\n",
    "        \n",
    "        return actions_batch, torch.stack(log_probs)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Convert Target Unitary to Network Input\n",
    "# ---------------------------\n",
    "def target_unitary_to_tensor(U):\n",
    "    \"\"\"\n",
    "    Convert U (a complex matrix) into a real tensor that concatenates the flattened real and imaginary parts.\n",
    "    \"\"\"\n",
    "    U_real = np.real(U).flatten()\n",
    "    U_imag = np.imag(U).flatten()\n",
    "    U_concat = np.concatenate([U_real, U_imag])\n",
    "    return torch.tensor(U_concat, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop (REINFORCE)\n",
    "# ---------------------------\n",
    "def train_policy(num_episodes=1000, lr=1e-3, print_every=100):\n",
    "    # Input dimension: dim*dim*2 (for real and imaginary parts).\n",
    "    input_dim = dim * dim * 2\n",
    "    hidden_dim = 128\n",
    "    policy_net = PolicyNetworkLSTM(input_dim, hidden_dim, sequence_length, num_allowed_couplings)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    # Running baseline for variance reduction.\n",
    "    baseline = None\n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate a target unitary.\n",
    "        U_target, true_params = generate_random_target_unitary()\n",
    "        target_tensor = target_unitary_to_tensor(U_target).unsqueeze(0)  # shape: (1, input_dim)\n",
    "        \n",
    "        # Forward pass: sample a candidate pulse sequence.\n",
    "        actions_batch, log_prob = policy_net(target_tensor)\n",
    "        actions = actions_batch[0]  # Assume batch_size=1.\n",
    "        \n",
    "        # Build candidate sequence from actions.\n",
    "        candidate_couplings = []\n",
    "        candidate_fractions = []\n",
    "        candidate_fixed_flags = []\n",
    "        for action in actions:\n",
    "            candidate_couplings.append(allowed_couplings[action['coupling']])\n",
    "            candidate_fractions.append(action['fraction'])\n",
    "            candidate_fixed_flags.append(action['fixed_phase_flag'])\n",
    "        \n",
    "        # Optionally apply the fix on couplings/phases.\n",
    "        candidate_couplings, candidate_fixed_flags = fix_couplings_and_phases(candidate_couplings, candidate_fixed_flags)\n",
    "        U_pred = unitary(candidate_couplings, rabi_freqs, candidate_fractions, candidate_fixed_flags, dim)\n",
    "        \n",
    "        # Compute reward based on fidelity.\n",
    "        reward = compute_fidelity(U_target, U_pred, dim)\n",
    "        reward_history.append(reward)\n",
    "        \n",
    "        # Update running baseline.\n",
    "        if baseline is None:\n",
    "            baseline = reward\n",
    "        else:\n",
    "            baseline = 0.99 * baseline + 0.01 * reward\n",
    "        \n",
    "        # REINFORCE loss: maximize reward by minimizing -log_prob * (reward - baseline)\n",
    "        loss = -log_prob * (reward - baseline)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(reward_history[-print_every:])\n",
    "            print(f\"Episode {episode+1}, Loss: {loss.item():.4f}, Reward: {reward:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
    "    \n",
    "    return policy_net\n",
    "\n",
    "# ---------------------------\n",
    "# Testing the Trained Policy\n",
    "# ---------------------------\n",
    "def test_policy(policy_net, num_tests=10):\n",
    "    test_rewards = []\n",
    "    for i in range(num_tests):\n",
    "        U_target, true_params = generate_random_target_unitary()\n",
    "        target_tensor = target_unitary_to_tensor(U_target).unsqueeze(0)\n",
    "        actions_batch, _ = policy_net(target_tensor)\n",
    "        actions = actions_batch[0]\n",
    "        candidate_couplings = []\n",
    "        candidate_fractions = []\n",
    "        candidate_fixed_flags = []\n",
    "        for action in actions:\n",
    "            candidate_couplings.append(allowed_couplings[action['coupling']])\n",
    "            candidate_fractions.append(action['fraction'])\n",
    "            candidate_fixed_flags.append(action['fixed_phase_flag'])\n",
    "        candidate_couplings, candidate_fixed_flags = fix_couplings_and_phases(candidate_couplings, candidate_fixed_flags)\n",
    "        U_pred = unitary(candidate_couplings, rabi_freqs, candidate_fractions, candidate_fixed_flags, dim)\n",
    "        reward = compute_fidelity(U_target, U_pred, dim)\n",
    "        test_rewards.append(reward)\n",
    "        print(f\"Test {i+1}: Fidelity = {reward:.4f}\")\n",
    "    print(f\"Average test fidelity: {np.mean(test_rewards):.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Train the LSTM-based policy network using the RL approach.\n",
    "    policy_net = train_policy(num_episodes=1000, lr=1e-3, print_every=100)\n",
    "    # Evaluate on a test set.\n",
    "    test_policy(policy_net, num_tests=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2239fe-caf7-4b42-be8c-25c9e02c05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /opt/anaconda3/lib/python3.12/site-packages (from scipy) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.6.0-cp312-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b7ab13-3714-48e9-b2c3-5c4d587b446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000, Loss: 0.0012, Reward: 0.1923, Avg Reward: 0.2294\n",
      "Episode 2000, Loss: 0.0013, Reward: 0.1060, Avg Reward: 0.2294\n",
      "Episode 3000, Loss: 0.0001, Reward: 0.2621, Avg Reward: 0.2384\n",
      "Episode 4000, Loss: -0.0001, Reward: 0.2055, Avg Reward: 0.2357\n",
      "Episode 5000, Loss: -3.3969, Reward: 0.6592, Avg Reward: 0.2371\n",
      "Episode 6000, Loss: 0.8371, Reward: 0.0722, Avg Reward: 0.2394\n",
      "Episode 7000, Loss: -0.1080, Reward: 0.1910, Avg Reward: 0.2324\n",
      "Episode 8000, Loss: -0.0922, Reward: 0.2708, Avg Reward: 0.2333\n",
      "Episode 9000, Loss: 0.0000, Reward: 0.2554, Avg Reward: 0.2322\n",
      "Episode 10000, Loss: -0.0000, Reward: 0.0262, Avg Reward: 0.2323\n",
      "Episode 11000, Loss: -0.2179, Reward: 0.0219, Avg Reward: 0.2373\n",
      "Episode 12000, Loss: -0.0000, Reward: 0.0721, Avg Reward: 0.2413\n",
      "Episode 13000, Loss: 0.0000, Reward: 0.6465, Avg Reward: 0.2398\n",
      "Episode 14000, Loss: 0.0000, Reward: 0.1343, Avg Reward: 0.2326\n",
      "Episode 15000, Loss: -0.0000, Reward: 0.1369, Avg Reward: 0.2288\n",
      "Episode 16000, Loss: -0.0000, Reward: 0.7115, Avg Reward: 0.2335\n",
      "Episode 17000, Loss: 0.0000, Reward: 0.1429, Avg Reward: 0.2281\n",
      "Episode 18000, Loss: 1.2834, Reward: 0.1359, Avg Reward: 0.2298\n",
      "Episode 19000, Loss: -0.0003, Reward: 0.2032, Avg Reward: 0.2272\n",
      "Episode 20000, Loss: 0.2231, Reward: 0.3512, Avg Reward: 0.2215\n",
      "Episode 21000, Loss: -4.4144, Reward: 0.4048, Avg Reward: 0.2328\n",
      "Episode 22000, Loss: -3.4336, Reward: 0.6258, Avg Reward: 0.2282\n",
      "Episode 23000, Loss: 0.0000, Reward: 0.2485, Avg Reward: 0.2340\n",
      "Episode 24000, Loss: 0.0000, Reward: 0.1956, Avg Reward: 0.2434\n",
      "Episode 25000, Loss: -0.0179, Reward: 0.1678, Avg Reward: 0.2438\n",
      "Episode 26000, Loss: 1.5565, Reward: 0.1044, Avg Reward: 0.2443\n",
      "Episode 27000, Loss: -0.5237, Reward: 0.4391, Avg Reward: 0.2453\n",
      "Episode 28000, Loss: 0.9823, Reward: 0.1274, Avg Reward: 0.2384\n",
      "Episode 29000, Loss: -0.0000, Reward: 0.1243, Avg Reward: 0.2397\n",
      "Episode 30000, Loss: -0.0305, Reward: 0.2639, Avg Reward: 0.2305\n",
      "Episode 31000, Loss: -0.0596, Reward: 0.0005, Avg Reward: 0.2408\n",
      "Episode 32000, Loss: -0.0000, Reward: 0.1761, Avg Reward: 0.2477\n",
      "Episode 33000, Loss: -0.0000, Reward: 0.0090, Avg Reward: 0.2373\n",
      "Episode 34000, Loss: 0.6098, Reward: 0.2325, Avg Reward: 0.2419\n",
      "Episode 35000, Loss: -0.4603, Reward: 0.1591, Avg Reward: 0.2437\n",
      "Episode 36000, Loss: -0.0000, Reward: 0.2604, Avg Reward: 0.2334\n",
      "Episode 37000, Loss: -0.0000, Reward: 0.1767, Avg Reward: 0.2474\n",
      "Episode 38000, Loss: -0.0000, Reward: 0.5567, Avg Reward: 0.2368\n",
      "Episode 39000, Loss: -0.0000, Reward: 0.3296, Avg Reward: 0.2464\n",
      "Episode 40000, Loss: -0.0000, Reward: 0.4505, Avg Reward: 0.2440\n",
      "Episode 41000, Loss: -10.7078, Reward: 0.5386, Avg Reward: 0.2395\n",
      "Episode 42000, Loss: -0.0000, Reward: 0.1422, Avg Reward: 0.2458\n",
      "Episode 43000, Loss: -8.1734, Reward: 0.4820, Avg Reward: 0.2405\n",
      "Episode 44000, Loss: -0.5333, Reward: 0.2438, Avg Reward: 0.2321\n",
      "Episode 45000, Loss: 0.0000, Reward: 0.0779, Avg Reward: 0.2391\n",
      "Episode 46000, Loss: -0.0000, Reward: 0.3652, Avg Reward: 0.2486\n",
      "Episode 47000, Loss: 0.0000, Reward: 0.1765, Avg Reward: 0.2459\n",
      "Episode 48000, Loss: -8.9470, Reward: 0.4899, Avg Reward: 0.2422\n",
      "Episode 49000, Loss: -2.4844, Reward: 0.3358, Avg Reward: 0.2469\n",
      "Episode 50000, Loss: 1.7258, Reward: 0.0324, Avg Reward: 0.2328\n",
      "Episode 51000, Loss: -0.0016, Reward: 0.0822, Avg Reward: 0.2295\n",
      "Episode 52000, Loss: -0.0000, Reward: 0.0431, Avg Reward: 0.2377\n",
      "Episode 53000, Loss: 0.0000, Reward: 0.0105, Avg Reward: 0.2397\n",
      "Episode 54000, Loss: 0.0000, Reward: 0.0614, Avg Reward: 0.2450\n",
      "Episode 55000, Loss: -0.0000, Reward: 0.3135, Avg Reward: 0.2390\n",
      "Episode 56000, Loss: 0.0000, Reward: 0.1682, Avg Reward: 0.2433\n",
      "Episode 57000, Loss: 0.0000, Reward: 0.0037, Avg Reward: 0.2462\n",
      "Episode 58000, Loss: -0.0000, Reward: 0.2013, Avg Reward: 0.2434\n",
      "Episode 59000, Loss: 0.0000, Reward: 0.5727, Avg Reward: 0.2445\n",
      "Episode 60000, Loss: -0.0011, Reward: 0.0200, Avg Reward: 0.2442\n",
      "Episode 61000, Loss: -1.1326, Reward: 0.2638, Avg Reward: 0.2428\n",
      "Episode 62000, Loss: 0.0000, Reward: 0.0408, Avg Reward: 0.2467\n",
      "Episode 63000, Loss: 0.0000, Reward: 0.0814, Avg Reward: 0.2293\n",
      "Episode 64000, Loss: -5.7045, Reward: 0.3588, Avg Reward: 0.2449\n",
      "Episode 65000, Loss: 0.0000, Reward: 0.0124, Avg Reward: 0.2279\n",
      "Episode 66000, Loss: 0.0000, Reward: 0.0976, Avg Reward: 0.2402\n",
      "Episode 67000, Loss: 0.0000, Reward: 0.1598, Avg Reward: 0.2406\n",
      "Episode 68000, Loss: -0.0000, Reward: 0.3243, Avg Reward: 0.2498\n",
      "Episode 69000, Loss: -0.3468, Reward: 0.3452, Avg Reward: 0.2454\n",
      "Episode 70000, Loss: -0.0000, Reward: 0.4513, Avg Reward: 0.2426\n",
      "Episode 71000, Loss: 0.0000, Reward: 0.1482, Avg Reward: 0.2406\n",
      "Episode 72000, Loss: -0.0000, Reward: 0.3512, Avg Reward: 0.2555\n",
      "Episode 73000, Loss: -10.4853, Reward: 0.4705, Avg Reward: 0.2516\n",
      "Episode 74000, Loss: -5.5045, Reward: 0.3731, Avg Reward: 0.2528\n",
      "Episode 75000, Loss: 0.0000, Reward: 0.0977, Avg Reward: 0.2373\n",
      "Episode 76000, Loss: -0.0000, Reward: 0.2683, Avg Reward: 0.2417\n",
      "Episode 77000, Loss: 0.0000, Reward: 0.1120, Avg Reward: 0.2421\n",
      "Episode 78000, Loss: 0.0000, Reward: 0.0578, Avg Reward: 0.2400\n",
      "Episode 79000, Loss: -0.0000, Reward: 0.3661, Avg Reward: 0.2361\n",
      "Episode 80000, Loss: -3.8189, Reward: 0.3350, Avg Reward: 0.2530\n",
      "Episode 81000, Loss: -0.0000, Reward: 0.7596, Avg Reward: 0.2384\n",
      "Episode 82000, Loss: -0.0000, Reward: 0.3241, Avg Reward: 0.2361\n",
      "Episode 83000, Loss: -0.0000, Reward: 0.4978, Avg Reward: 0.2408\n",
      "Episode 84000, Loss: 0.0000, Reward: 0.0564, Avg Reward: 0.2384\n",
      "Episode 85000, Loss: -0.0000, Reward: 0.6390, Avg Reward: 0.2450\n",
      "Episode 86000, Loss: 0.0000, Reward: 0.1988, Avg Reward: 0.2466\n",
      "Episode 87000, Loss: 0.0000, Reward: 0.0766, Avg Reward: 0.2530\n",
      "Episode 88000, Loss: 0.0000, Reward: 0.1279, Avg Reward: 0.2430\n",
      "Episode 89000, Loss: -0.0000, Reward: 0.3131, Avg Reward: 0.2466\n",
      "Episode 90000, Loss: -0.0000, Reward: 0.3937, Avg Reward: 0.2483\n",
      "Episode 91000, Loss: 0.0000, Reward: 0.1654, Avg Reward: 0.2409\n",
      "Episode 92000, Loss: 0.0000, Reward: 0.2093, Avg Reward: 0.2515\n",
      "Episode 93000, Loss: 5.7224, Reward: 0.1548, Avg Reward: 0.2449\n",
      "Episode 94000, Loss: -0.0000, Reward: 0.2802, Avg Reward: 0.2423\n",
      "Episode 95000, Loss: 0.0000, Reward: 0.0502, Avg Reward: 0.2454\n",
      "Episode 96000, Loss: 0.0000, Reward: 0.0552, Avg Reward: 0.2469\n",
      "Episode 97000, Loss: -0.0000, Reward: 0.3590, Avg Reward: 0.2482\n",
      "Episode 98000, Loss: -0.0000, Reward: 0.4031, Avg Reward: 0.2413\n",
      "Episode 99000, Loss: -0.0000, Reward: 0.2672, Avg Reward: 0.2408\n",
      "Episode 100000, Loss: 0.0000, Reward: 0.1231, Avg Reward: 0.2443\n",
      "Episode 101000, Loss: 0.0000, Reward: 0.2097, Avg Reward: 0.2382\n",
      "Episode 102000, Loss: 0.0000, Reward: 0.1087, Avg Reward: 0.2362\n",
      "Episode 103000, Loss: 0.0000, Reward: 0.1792, Avg Reward: 0.2395\n",
      "Episode 104000, Loss: -0.0000, Reward: 0.2679, Avg Reward: 0.2404\n",
      "Episode 105000, Loss: -0.0000, Reward: 0.3913, Avg Reward: 0.2366\n",
      "Episode 106000, Loss: -0.0000, Reward: 0.2404, Avg Reward: 0.2337\n",
      "Episode 107000, Loss: -4.1147, Reward: 0.3269, Avg Reward: 0.2421\n",
      "Episode 108000, Loss: -0.0041, Reward: 0.0566, Avg Reward: 0.2499\n",
      "Episode 109000, Loss: 0.0000, Reward: 0.0315, Avg Reward: 0.2414\n",
      "Episode 110000, Loss: 0.0000, Reward: 0.4539, Avg Reward: 0.2434\n",
      "Episode 111000, Loss: 0.0000, Reward: 0.0962, Avg Reward: 0.2407\n",
      "Episode 112000, Loss: -0.0000, Reward: 0.3383, Avg Reward: 0.2343\n",
      "Episode 113000, Loss: 0.0000, Reward: 0.1315, Avg Reward: 0.2416\n",
      "Episode 114000, Loss: 0.0000, Reward: 0.2113, Avg Reward: 0.2475\n",
      "Episode 115000, Loss: 0.0000, Reward: 0.0190, Avg Reward: 0.2458\n",
      "Episode 116000, Loss: 0.0000, Reward: 0.4804, Avg Reward: 0.2459\n",
      "Episode 117000, Loss: 0.0000, Reward: 0.2099, Avg Reward: 0.2365\n",
      "Episode 118000, Loss: 0.0000, Reward: 0.3578, Avg Reward: 0.2449\n",
      "Episode 119000, Loss: -0.0000, Reward: 0.1461, Avg Reward: 0.2378\n",
      "Episode 120000, Loss: -11.1605, Reward: 0.4411, Avg Reward: 0.2379\n",
      "Episode 121000, Loss: 0.0000, Reward: 0.4473, Avg Reward: 0.2410\n",
      "Episode 122000, Loss: 0.0000, Reward: 0.1075, Avg Reward: 0.2530\n",
      "Episode 123000, Loss: 0.0000, Reward: 0.0511, Avg Reward: 0.2468\n",
      "Episode 124000, Loss: 0.0000, Reward: 0.2009, Avg Reward: 0.2409\n",
      "Episode 125000, Loss: -13.6769, Reward: 0.4823, Avg Reward: 0.2431\n",
      "Episode 126000, Loss: 0.0000, Reward: 0.1315, Avg Reward: 0.2421\n",
      "Episode 127000, Loss: 0.0000, Reward: 0.0381, Avg Reward: 0.2438\n",
      "Episode 128000, Loss: -0.0000, Reward: 0.1187, Avg Reward: 0.2389\n",
      "Episode 129000, Loss: -2.6189, Reward: 0.2829, Avg Reward: 0.2424\n",
      "Episode 130000, Loss: 0.0000, Reward: 0.0859, Avg Reward: 0.2486\n",
      "Episode 131000, Loss: 0.0334, Reward: 0.2461, Avg Reward: 0.2432\n",
      "Episode 132000, Loss: 0.0000, Reward: 0.0925, Avg Reward: 0.2561\n",
      "Episode 133000, Loss: 0.0000, Reward: 0.0665, Avg Reward: 0.2508\n",
      "Episode 134000, Loss: 0.0000, Reward: 0.1358, Avg Reward: 0.2504\n",
      "Episode 135000, Loss: -0.0000, Reward: 0.5860, Avg Reward: 0.2608\n",
      "Episode 136000, Loss: -0.8466, Reward: 0.2581, Avg Reward: 0.2366\n",
      "Episode 137000, Loss: 6.3572, Reward: 0.0666, Avg Reward: 0.2420\n",
      "Episode 138000, Loss: -0.0000, Reward: 0.3104, Avg Reward: 0.2485\n",
      "Episode 139000, Loss: -3.7839, Reward: 0.3195, Avg Reward: 0.2444\n",
      "Episode 140000, Loss: 0.0000, Reward: 0.2052, Avg Reward: 0.2502\n",
      "Episode 141000, Loss: -6.5136, Reward: 0.3705, Avg Reward: 0.2488\n",
      "Episode 142000, Loss: -0.0000, Reward: 0.5385, Avg Reward: 0.2343\n",
      "Episode 143000, Loss: 0.0000, Reward: 0.0121, Avg Reward: 0.2485\n",
      "Episode 144000, Loss: 2.0352, Reward: 0.0865, Avg Reward: 0.2424\n",
      "Episode 145000, Loss: -0.0000, Reward: 0.7996, Avg Reward: 0.2475\n",
      "Episode 146000, Loss: 0.0000, Reward: 0.2113, Avg Reward: 0.2480\n",
      "Episode 147000, Loss: 0.0000, Reward: 0.1022, Avg Reward: 0.2521\n",
      "Episode 148000, Loss: -0.0000, Reward: 0.3559, Avg Reward: 0.2484\n",
      "Episode 149000, Loss: 1.5787, Reward: 0.0615, Avg Reward: 0.2388\n",
      "Episode 150000, Loss: 0.0000, Reward: 0.0711, Avg Reward: 0.2449\n",
      "Episode 151000, Loss: 0.0000, Reward: 0.1966, Avg Reward: 0.2404\n",
      "Episode 152000, Loss: 0.0000, Reward: 0.2338, Avg Reward: 0.2442\n",
      "Episode 153000, Loss: 1.2545, Reward: 0.1209, Avg Reward: 0.2402\n",
      "Episode 154000, Loss: 7.3862, Reward: 0.0269, Avg Reward: 0.2576\n",
      "Episode 155000, Loss: -0.9685, Reward: 0.2805, Avg Reward: 0.2573\n",
      "Episode 156000, Loss: -12.4994, Reward: 0.5758, Avg Reward: 0.2533\n",
      "Episode 157000, Loss: -0.3912, Reward: 0.2777, Avg Reward: 0.2364\n",
      "Episode 158000, Loss: 4.3159, Reward: 0.1127, Avg Reward: 0.2604\n",
      "Episode 159000, Loss: -4.6336, Reward: 0.3279, Avg Reward: 0.2458\n",
      "Episode 160000, Loss: -16.2433, Reward: 0.5320, Avg Reward: 0.2538\n",
      "Episode 161000, Loss: -4.6967, Reward: 0.4234, Avg Reward: 0.2439\n",
      "Episode 162000, Loss: 0.4928, Reward: 0.2188, Avg Reward: 0.2397\n",
      "Episode 163000, Loss: 1.4535, Reward: 0.1250, Avg Reward: 0.2461\n",
      "Episode 164000, Loss: -0.0000, Reward: 0.5256, Avg Reward: 0.2487\n",
      "Episode 165000, Loss: 0.6021, Reward: 0.2215, Avg Reward: 0.2542\n",
      "Episode 166000, Loss: 1.3458, Reward: 0.1945, Avg Reward: 0.2461\n",
      "Episode 167000, Loss: 2.5721, Reward: 0.1493, Avg Reward: 0.2504\n",
      "Episode 168000, Loss: 0.0000, Reward: 0.1325, Avg Reward: 0.2432\n",
      "Episode 169000, Loss: 1.3924, Reward: 0.1111, Avg Reward: 0.2408\n",
      "Episode 170000, Loss: -0.0000, Reward: 0.0433, Avg Reward: 0.2452\n",
      "Episode 171000, Loss: -9.1105, Reward: 0.4645, Avg Reward: 0.2442\n",
      "Episode 172000, Loss: 0.0000, Reward: 0.1341, Avg Reward: 0.2512\n",
      "Episode 173000, Loss: 0.0000, Reward: 0.1781, Avg Reward: 0.2377\n",
      "Episode 174000, Loss: 3.0648, Reward: 0.1677, Avg Reward: 0.2388\n",
      "Episode 175000, Loss: -0.0000, Reward: 0.2375, Avg Reward: 0.2422\n",
      "Episode 176000, Loss: 0.0000, Reward: 0.1038, Avg Reward: 0.2448\n",
      "Episode 177000, Loss: -0.2824, Reward: 0.3119, Avg Reward: 0.2408\n",
      "Episode 178000, Loss: 0.0000, Reward: 0.2034, Avg Reward: 0.2409\n",
      "Episode 179000, Loss: -0.0000, Reward: 0.2574, Avg Reward: 0.2405\n",
      "Episode 180000, Loss: 0.0000, Reward: 0.1529, Avg Reward: 0.2437\n",
      "Episode 181000, Loss: 0.0000, Reward: 0.1534, Avg Reward: 0.2482\n",
      "Episode 182000, Loss: 0.3089, Reward: 0.2006, Avg Reward: 0.2410\n",
      "Episode 183000, Loss: 0.0000, Reward: 0.2017, Avg Reward: 0.2356\n",
      "Episode 184000, Loss: 0.0000, Reward: 0.1277, Avg Reward: 0.2437\n",
      "Episode 185000, Loss: -0.0000, Reward: 0.2644, Avg Reward: 0.2418\n",
      "Episode 186000, Loss: 0.0000, Reward: 0.1270, Avg Reward: 0.2410\n",
      "Episode 187000, Loss: 0.0000, Reward: 0.1291, Avg Reward: 0.2449\n",
      "Episode 188000, Loss: -0.0000, Reward: 0.2554, Avg Reward: 0.2476\n",
      "Episode 189000, Loss: 0.6961, Reward: 0.2371, Avg Reward: 0.2445\n",
      "Episode 190000, Loss: 1.9941, Reward: 0.1683, Avg Reward: 0.2420\n",
      "Episode 191000, Loss: 0.0000, Reward: 0.1850, Avg Reward: 0.2356\n",
      "Episode 192000, Loss: 0.0000, Reward: 0.1996, Avg Reward: 0.2427\n",
      "Episode 193000, Loss: -0.0000, Reward: 0.2600, Avg Reward: 0.2387\n",
      "Episode 194000, Loss: -18.1788, Reward: 0.5534, Avg Reward: 0.2501\n",
      "Episode 195000, Loss: -0.8572, Reward: 0.3841, Avg Reward: 0.2367\n",
      "Episode 196000, Loss: -0.0000, Reward: 0.6424, Avg Reward: 0.2454\n",
      "Episode 197000, Loss: 0.0000, Reward: 0.2109, Avg Reward: 0.2446\n",
      "Episode 198000, Loss: -0.0000, Reward: 0.1111, Avg Reward: 0.2449\n",
      "Episode 199000, Loss: 1.4196, Reward: 0.1575, Avg Reward: 0.2558\n",
      "Episode 200000, Loss: 0.0001, Reward: 0.1885, Avg Reward: 0.2501\n",
      "Episode 201000, Loss: 1.4309, Reward: 0.1249, Avg Reward: 0.2429\n",
      "Episode 202000, Loss: -1.3610, Reward: 0.2942, Avg Reward: 0.2349\n",
      "Episode 203000, Loss: 0.0000, Reward: 0.0140, Avg Reward: 0.2416\n",
      "Episode 204000, Loss: -2.1026, Reward: 0.3625, Avg Reward: 0.2415\n",
      "Episode 205000, Loss: -0.0000, Reward: 0.4932, Avg Reward: 0.2449\n",
      "Episode 206000, Loss: -0.0000, Reward: 0.2373, Avg Reward: 0.2396\n",
      "Episode 207000, Loss: 0.0000, Reward: 0.1710, Avg Reward: 0.2477\n",
      "Episode 208000, Loss: 0.0000, Reward: 0.1514, Avg Reward: 0.2448\n",
      "Episode 209000, Loss: -6.4052, Reward: 0.5230, Avg Reward: 0.2445\n",
      "Episode 210000, Loss: -1.0635, Reward: 0.2812, Avg Reward: 0.2425\n",
      "Episode 211000, Loss: 2.2316, Reward: 0.1491, Avg Reward: 0.2460\n",
      "Episode 212000, Loss: -8.8311, Reward: 0.5346, Avg Reward: 0.2504\n",
      "Episode 213000, Loss: 0.0000, Reward: 0.1377, Avg Reward: 0.2399\n",
      "Episode 214000, Loss: -7.2279, Reward: 0.4788, Avg Reward: 0.2438\n",
      "Episode 215000, Loss: -10.3762, Reward: 0.4075, Avg Reward: 0.2407\n",
      "Episode 216000, Loss: -21.7971, Reward: 0.7450, Avg Reward: 0.2378\n",
      "Episode 217000, Loss: 2.0987, Reward: 0.0727, Avg Reward: 0.2439\n",
      "Episode 218000, Loss: 0.0000, Reward: 0.0309, Avg Reward: 0.2431\n",
      "Episode 219000, Loss: 0.1133, Reward: 0.1552, Avg Reward: 0.2531\n",
      "Episode 220000, Loss: -2.5650, Reward: 0.3339, Avg Reward: 0.2371\n",
      "Episode 221000, Loss: -13.3872, Reward: 0.4484, Avg Reward: 0.2373\n",
      "Episode 222000, Loss: -4.8286, Reward: 0.3255, Avg Reward: 0.2535\n",
      "Episode 223000, Loss: -0.0846, Reward: 0.2604, Avg Reward: 0.2497\n",
      "Episode 224000, Loss: -6.4881, Reward: 0.3467, Avg Reward: 0.2407\n",
      "Episode 225000, Loss: -0.0319, Reward: 0.1420, Avg Reward: 0.2421\n",
      "Episode 226000, Loss: -0.0000, Reward: 0.2472, Avg Reward: 0.2433\n",
      "Episode 227000, Loss: 0.6667, Reward: 0.2070, Avg Reward: 0.2384\n",
      "Episode 228000, Loss: 4.8833, Reward: 0.1426, Avg Reward: 0.2479\n",
      "Episode 229000, Loss: -0.0000, Reward: 0.3898, Avg Reward: 0.2500\n",
      "Episode 230000, Loss: 1.4211, Reward: 0.1910, Avg Reward: 0.2458\n",
      "Episode 231000, Loss: 0.0000, Reward: 0.1018, Avg Reward: 0.2510\n",
      "Episode 232000, Loss: 0.0000, Reward: 0.0667, Avg Reward: 0.2418\n",
      "Episode 233000, Loss: -1.2195, Reward: 0.4803, Avg Reward: 0.2525\n",
      "Episode 234000, Loss: 0.0000, Reward: 0.0031, Avg Reward: 0.2510\n",
      "Episode 235000, Loss: 0.0000, Reward: 0.1626, Avg Reward: 0.2528\n",
      "Episode 236000, Loss: 0.0534, Reward: 0.2344, Avg Reward: 0.2423\n",
      "Episode 237000, Loss: -0.0105, Reward: 0.2380, Avg Reward: 0.2376\n",
      "Episode 238000, Loss: 0.0000, Reward: 0.0737, Avg Reward: 0.2506\n",
      "Episode 239000, Loss: -0.0000, Reward: 0.4400, Avg Reward: 0.2379\n",
      "Episode 240000, Loss: -1.8325, Reward: 0.3010, Avg Reward: 0.2507\n",
      "Episode 241000, Loss: -0.0000, Reward: 0.3202, Avg Reward: 0.2543\n",
      "Episode 242000, Loss: 0.0000, Reward: 0.0054, Avg Reward: 0.2480\n",
      "Episode 243000, Loss: 0.0000, Reward: 0.0009, Avg Reward: 0.2334\n",
      "Episode 244000, Loss: 0.0000, Reward: 0.2273, Avg Reward: 0.2459\n",
      "Episode 245000, Loss: 1.7353, Reward: 0.2080, Avg Reward: 0.2496\n",
      "Episode 246000, Loss: 0.0000, Reward: 0.1987, Avg Reward: 0.2598\n",
      "Episode 247000, Loss: -18.1869, Reward: 0.6202, Avg Reward: 0.2530\n",
      "Episode 248000, Loss: -15.5493, Reward: 0.5640, Avg Reward: 0.2460\n",
      "Episode 249000, Loss: -0.0000, Reward: 0.2546, Avg Reward: 0.2448\n",
      "Episode 250000, Loss: 0.0000, Reward: 0.1481, Avg Reward: 0.2468\n",
      "Episode 251000, Loss: -1.3285, Reward: 0.2633, Avg Reward: 0.2443\n",
      "Episode 252000, Loss: 0.0000, Reward: 0.0382, Avg Reward: 0.2537\n",
      "Episode 253000, Loss: -0.1332, Reward: 0.1528, Avg Reward: 0.2452\n",
      "Episode 254000, Loss: -13.6031, Reward: 0.4621, Avg Reward: 0.2547\n",
      "Episode 255000, Loss: 0.0000, Reward: 0.0527, Avg Reward: 0.2418\n",
      "Episode 256000, Loss: 1.0236, Reward: 0.2009, Avg Reward: 0.2397\n",
      "Episode 257000, Loss: -9.9764, Reward: 0.4215, Avg Reward: 0.2474\n",
      "Episode 258000, Loss: 0.0000, Reward: 0.0472, Avg Reward: 0.2527\n",
      "Episode 259000, Loss: -8.5315, Reward: 0.3865, Avg Reward: 0.2430\n",
      "Episode 260000, Loss: -0.0000, Reward: 0.2925, Avg Reward: 0.2646\n",
      "Episode 261000, Loss: 0.8121, Reward: 0.0968, Avg Reward: 0.2584\n",
      "Episode 262000, Loss: -0.0000, Reward: 0.4848, Avg Reward: 0.2589\n",
      "Episode 263000, Loss: -2.8959, Reward: 0.3042, Avg Reward: 0.2477\n",
      "Episode 264000, Loss: -0.0000, Reward: 0.0400, Avg Reward: 0.2592\n",
      "Episode 265000, Loss: -0.0000, Reward: 0.2286, Avg Reward: 0.2609\n",
      "Episode 266000, Loss: 0.0000, Reward: 0.0916, Avg Reward: 0.2633\n",
      "Episode 267000, Loss: 0.0001, Reward: 0.2957, Avg Reward: 0.2546\n",
      "Episode 268000, Loss: -0.0020, Reward: 0.2358, Avg Reward: 0.2566\n",
      "Episode 269000, Loss: -0.0000, Reward: 0.2973, Avg Reward: 0.2588\n",
      "Episode 270000, Loss: -18.1837, Reward: 0.5480, Avg Reward: 0.2632\n",
      "Episode 271000, Loss: 0.0000, Reward: 0.2606, Avg Reward: 0.2629\n",
      "Episode 272000, Loss: 0.0006, Reward: 0.5712, Avg Reward: 0.2643\n",
      "Episode 273000, Loss: -0.0000, Reward: 0.2378, Avg Reward: 0.2366\n",
      "Episode 274000, Loss: 0.0000, Reward: 0.1447, Avg Reward: 0.2528\n",
      "Episode 275000, Loss: 0.2991, Reward: 0.2283, Avg Reward: 0.2376\n",
      "Episode 276000, Loss: -13.0914, Reward: 0.4733, Avg Reward: 0.2573\n",
      "Episode 277000, Loss: -0.0034, Reward: 0.0731, Avg Reward: 0.2498\n",
      "Episode 278000, Loss: -0.0000, Reward: 0.0667, Avg Reward: 0.2382\n",
      "Episode 279000, Loss: -0.0000, Reward: 0.1735, Avg Reward: 0.2374\n",
      "Episode 280000, Loss: -0.0009, Reward: 0.1353, Avg Reward: 0.2555\n",
      "Episode 281000, Loss: -0.0137, Reward: 0.2189, Avg Reward: 0.2607\n",
      "Episode 282000, Loss: 0.0000, Reward: 0.3515, Avg Reward: 0.2557\n",
      "Episode 283000, Loss: -8.1402, Reward: 0.4283, Avg Reward: 0.2672\n",
      "Episode 284000, Loss: -0.0000, Reward: 0.3243, Avg Reward: 0.2576\n",
      "Episode 285000, Loss: 5.1685, Reward: 0.1648, Avg Reward: 0.2560\n",
      "Episode 286000, Loss: -0.0000, Reward: 0.1793, Avg Reward: 0.2530\n",
      "Episode 287000, Loss: 0.0000, Reward: 0.0371, Avg Reward: 0.2473\n",
      "Episode 288000, Loss: -26.2598, Reward: 0.7955, Avg Reward: 0.2379\n",
      "Episode 289000, Loss: 0.0000, Reward: 0.1960, Avg Reward: 0.2439\n",
      "Episode 290000, Loss: 1.0672, Reward: 0.2137, Avg Reward: 0.2464\n",
      "Episode 291000, Loss: -0.0000, Reward: 0.2603, Avg Reward: 0.2497\n",
      "Episode 292000, Loss: 0.0000, Reward: 0.0125, Avg Reward: 0.2435\n",
      "Episode 293000, Loss: -1.0588, Reward: 0.2911, Avg Reward: 0.2505\n",
      "Episode 294000, Loss: 0.0000, Reward: 0.1998, Avg Reward: 0.2501\n",
      "Episode 295000, Loss: -0.0000, Reward: 0.0774, Avg Reward: 0.2539\n",
      "Episode 296000, Loss: -25.1972, Reward: 0.7789, Avg Reward: 0.2506\n",
      "Episode 297000, Loss: 2.3270, Reward: 0.1347, Avg Reward: 0.2509\n",
      "Episode 298000, Loss: -0.0000, Reward: 0.1916, Avg Reward: 0.2524\n",
      "Episode 299000, Loss: 2.8892, Reward: 0.1570, Avg Reward: 0.2382\n",
      "Episode 300000, Loss: -19.9338, Reward: 0.5645, Avg Reward: 0.2559\n",
      "Episode 301000, Loss: -0.0001, Reward: 0.0324, Avg Reward: 0.2601\n",
      "Episode 302000, Loss: -5.0256, Reward: 0.3597, Avg Reward: 0.2700\n",
      "Episode 303000, Loss: -0.0000, Reward: 0.2111, Avg Reward: 0.2391\n",
      "Episode 304000, Loss: -11.6618, Reward: 0.4949, Avg Reward: 0.2655\n",
      "Episode 305000, Loss: -0.0000, Reward: 0.0818, Avg Reward: 0.2675\n",
      "Episode 306000, Loss: -0.0039, Reward: 0.1682, Avg Reward: 0.2614\n",
      "Episode 307000, Loss: 3.8933, Reward: 0.1936, Avg Reward: 0.2608\n",
      "Episode 308000, Loss: -0.0017, Reward: 0.0791, Avg Reward: 0.2554\n",
      "Episode 309000, Loss: -0.7061, Reward: 0.3220, Avg Reward: 0.2486\n",
      "Episode 310000, Loss: 0.0000, Reward: 0.1735, Avg Reward: 0.2519\n",
      "Episode 311000, Loss: 0.0000, Reward: 0.0642, Avg Reward: 0.2563\n",
      "Episode 312000, Loss: 0.0005, Reward: 0.4148, Avg Reward: 0.2558\n",
      "Episode 313000, Loss: 2.1676, Reward: 0.2074, Avg Reward: 0.2717\n",
      "Episode 314000, Loss: -17.8750, Reward: 0.6250, Avg Reward: 0.2530\n",
      "Episode 315000, Loss: -0.0000, Reward: 0.4365, Avg Reward: 0.2662\n",
      "Episode 316000, Loss: -2.6895, Reward: 0.3015, Avg Reward: 0.2440\n",
      "Episode 317000, Loss: -0.0115, Reward: 0.1026, Avg Reward: 0.2497\n",
      "Episode 318000, Loss: -1.1261, Reward: 0.2866, Avg Reward: 0.2562\n",
      "Episode 319000, Loss: -1.5364, Reward: 0.0263, Avg Reward: 0.2639\n",
      "Episode 320000, Loss: -3.9437, Reward: 0.4139, Avg Reward: 0.2389\n",
      "Episode 321000, Loss: 0.0006, Reward: 0.3542, Avg Reward: 0.2478\n",
      "Episode 322000, Loss: 0.6587, Reward: 0.2528, Avg Reward: 0.2695\n",
      "Episode 323000, Loss: -0.0125, Reward: 0.0224, Avg Reward: 0.2562\n",
      "Episode 324000, Loss: 1.8530, Reward: 0.2404, Avg Reward: 0.2655\n",
      "Episode 325000, Loss: -0.0001, Reward: 0.1175, Avg Reward: 0.2662\n",
      "Episode 326000, Loss: -2.3534, Reward: 0.2948, Avg Reward: 0.2559\n",
      "Episode 327000, Loss: -0.0001, Reward: 0.1400, Avg Reward: 0.2530\n",
      "Episode 328000, Loss: -12.8388, Reward: 0.5042, Avg Reward: 0.2605\n",
      "Episode 329000, Loss: 0.0311, Reward: 0.4192, Avg Reward: 0.2645\n",
      "Episode 330000, Loss: 0.6320, Reward: 0.2442, Avg Reward: 0.2426\n",
      "Episode 331000, Loss: -22.6525, Reward: 0.7116, Avg Reward: 0.2414\n",
      "Episode 332000, Loss: -0.6617, Reward: 0.0225, Avg Reward: 0.2554\n",
      "Episode 333000, Loss: -0.0000, Reward: 0.1358, Avg Reward: 0.2520\n",
      "Episode 334000, Loss: -0.0000, Reward: 0.0899, Avg Reward: 0.2555\n",
      "Episode 335000, Loss: -6.8723, Reward: 0.3976, Avg Reward: 0.2557\n",
      "Episode 336000, Loss: -0.0000, Reward: 0.2968, Avg Reward: 0.2555\n",
      "Episode 337000, Loss: -0.0076, Reward: 0.0533, Avg Reward: 0.2502\n",
      "Episode 338000, Loss: 0.0000, Reward: 0.1114, Avg Reward: 0.2502\n",
      "Episode 339000, Loss: -24.4180, Reward: 0.7123, Avg Reward: 0.2451\n",
      "Episode 340000, Loss: -4.0666, Reward: 0.3322, Avg Reward: 0.2549\n",
      "Episode 341000, Loss: -6.2853, Reward: 0.3612, Avg Reward: 0.2499\n",
      "Episode 342000, Loss: 0.8029, Reward: 0.2522, Avg Reward: 0.2548\n",
      "Episode 343000, Loss: 0.0000, Reward: 0.0008, Avg Reward: 0.2511\n",
      "Episode 344000, Loss: -10.1725, Reward: 0.6330, Avg Reward: 0.2621\n",
      "Episode 345000, Loss: -0.0000, Reward: 0.3168, Avg Reward: 0.2545\n",
      "Episode 346000, Loss: -0.0000, Reward: 0.1897, Avg Reward: 0.2419\n",
      "Episode 347000, Loss: -0.0170, Reward: 0.0482, Avg Reward: 0.2418\n",
      "Episode 348000, Loss: -9.6861, Reward: 0.4298, Avg Reward: 0.2432\n",
      "Episode 349000, Loss: 1.6013, Reward: 0.1962, Avg Reward: 0.2536\n",
      "Episode 350000, Loss: -13.1077, Reward: 0.4575, Avg Reward: 0.2443\n",
      "Episode 351000, Loss: -0.0000, Reward: 0.1253, Avg Reward: 0.2519\n",
      "Episode 352000, Loss: -0.0002, Reward: 0.1118, Avg Reward: 0.2517\n",
      "Episode 353000, Loss: -14.6979, Reward: 0.5194, Avg Reward: 0.2441\n",
      "Episode 354000, Loss: -0.0075, Reward: 0.0436, Avg Reward: 0.2501\n",
      "Episode 355000, Loss: -0.0034, Reward: 0.0999, Avg Reward: 0.2523\n",
      "Episode 356000, Loss: -0.0000, Reward: 0.1137, Avg Reward: 0.2569\n",
      "Episode 357000, Loss: 4.0194, Reward: 0.1809, Avg Reward: 0.2510\n",
      "Episode 358000, Loss: -6.9501, Reward: 0.3849, Avg Reward: 0.2546\n",
      "Episode 359000, Loss: 0.0815, Reward: 0.5650, Avg Reward: 0.2636\n",
      "Episode 360000, Loss: -0.0000, Reward: 0.3482, Avg Reward: 0.2752\n",
      "Episode 361000, Loss: -0.0000, Reward: 0.0214, Avg Reward: 0.2637\n",
      "Episode 362000, Loss: -0.0019, Reward: 0.2155, Avg Reward: 0.2650\n",
      "Episode 363000, Loss: -13.5135, Reward: 0.4828, Avg Reward: 0.2666\n",
      "Episode 364000, Loss: -2.3958, Reward: 0.4054, Avg Reward: 0.2733\n",
      "Episode 365000, Loss: 2.0980, Reward: 0.2316, Avg Reward: 0.2790\n",
      "Episode 366000, Loss: -21.2711, Reward: 0.6690, Avg Reward: 0.2635\n",
      "Episode 367000, Loss: -0.0000, Reward: 0.1777, Avg Reward: 0.2582\n",
      "Episode 368000, Loss: -14.3130, Reward: 0.5050, Avg Reward: 0.2635\n",
      "Episode 369000, Loss: 1.9277, Reward: 0.2151, Avg Reward: 0.2568\n",
      "Episode 370000, Loss: 6.3987, Reward: 0.0550, Avg Reward: 0.2669\n",
      "Episode 371000, Loss: 2.9979, Reward: 0.1059, Avg Reward: 0.2692\n",
      "Episode 372000, Loss: 1.9693, Reward: 0.2338, Avg Reward: 0.2649\n",
      "Episode 373000, Loss: 0.5167, Reward: 0.3656, Avg Reward: 0.2653\n",
      "Episode 374000, Loss: -0.0009, Reward: 0.0102, Avg Reward: 0.2749\n",
      "Episode 375000, Loss: -0.1041, Reward: 0.0494, Avg Reward: 0.2603\n",
      "Episode 376000, Loss: 0.0702, Reward: 0.0897, Avg Reward: 0.2686\n",
      "Episode 377000, Loss: -0.0002, Reward: 0.1238, Avg Reward: 0.2724\n",
      "Episode 378000, Loss: -0.0001, Reward: 0.0032, Avg Reward: 0.2701\n",
      "Episode 379000, Loss: -0.0002, Reward: 0.0306, Avg Reward: 0.2654\n",
      "Episode 380000, Loss: -29.6620, Reward: 0.8403, Avg Reward: 0.2727\n",
      "Episode 381000, Loss: -0.0261, Reward: 0.1416, Avg Reward: 0.2601\n",
      "Episode 382000, Loss: 0.0000, Reward: 0.1267, Avg Reward: 0.2665\n",
      "Episode 383000, Loss: -0.0001, Reward: 0.0617, Avg Reward: 0.2702\n",
      "Episode 384000, Loss: -0.2978, Reward: 0.0324, Avg Reward: 0.2624\n",
      "Episode 385000, Loss: -0.2429, Reward: 0.1655, Avg Reward: 0.2602\n",
      "Episode 386000, Loss: -0.0000, Reward: 0.0729, Avg Reward: 0.2510\n",
      "Episode 387000, Loss: 0.0000, Reward: 0.0271, Avg Reward: 0.2550\n",
      "Episode 388000, Loss: -3.7028, Reward: 0.3365, Avg Reward: 0.2536\n",
      "Episode 389000, Loss: -2.2275, Reward: 0.4457, Avg Reward: 0.2657\n",
      "Episode 390000, Loss: -0.0047, Reward: 0.0145, Avg Reward: 0.2655\n",
      "Episode 391000, Loss: -0.0000, Reward: 0.2081, Avg Reward: 0.2627\n",
      "Episode 392000, Loss: 1.2005, Reward: 0.1241, Avg Reward: 0.2603\n",
      "Episode 393000, Loss: -0.4647, Reward: 0.3003, Avg Reward: 0.2715\n",
      "Episode 394000, Loss: -1.9689, Reward: 0.6212, Avg Reward: 0.2613\n",
      "Episode 395000, Loss: -2.1773, Reward: 0.2974, Avg Reward: 0.2609\n",
      "Episode 396000, Loss: 0.0000, Reward: 0.0835, Avg Reward: 0.2582\n",
      "Episode 397000, Loss: 0.0000, Reward: 0.0425, Avg Reward: 0.2597\n",
      "Episode 398000, Loss: 0.0000, Reward: 0.0193, Avg Reward: 0.2595\n",
      "Episode 399000, Loss: -0.0315, Reward: 0.0210, Avg Reward: 0.2564\n",
      "Episode 400000, Loss: 0.0000, Reward: 0.1856, Avg Reward: 0.2594\n",
      "Episode 401000, Loss: -0.0000, Reward: 0.0059, Avg Reward: 0.2651\n",
      "Episode 402000, Loss: 0.0000, Reward: 0.2363, Avg Reward: 0.2579\n",
      "Episode 403000, Loss: 0.0005, Reward: 0.4060, Avg Reward: 0.2614\n",
      "Episode 404000, Loss: 0.0000, Reward: 0.2346, Avg Reward: 0.2554\n",
      "Episode 405000, Loss: -7.5545, Reward: 0.4172, Avg Reward: 0.2561\n",
      "Episode 406000, Loss: -18.7100, Reward: 0.6239, Avg Reward: 0.2668\n",
      "Episode 407000, Loss: 0.0000, Reward: 0.1166, Avg Reward: 0.2628\n",
      "Episode 408000, Loss: 0.0000, Reward: 0.3739, Avg Reward: 0.2626\n",
      "Episode 409000, Loss: 0.0000, Reward: 0.0767, Avg Reward: 0.2627\n",
      "Episode 410000, Loss: 0.5683, Reward: 0.2591, Avg Reward: 0.2686\n",
      "Episode 411000, Loss: 1.7049, Reward: 0.1866, Avg Reward: 0.2633\n",
      "Episode 412000, Loss: 0.0000, Reward: 0.0033, Avg Reward: 0.2645\n",
      "Episode 413000, Loss: -3.7316, Reward: 0.3278, Avg Reward: 0.2652\n",
      "Episode 414000, Loss: 0.0001, Reward: 0.3050, Avg Reward: 0.2757\n",
      "Episode 415000, Loss: -24.3558, Reward: 0.6985, Avg Reward: 0.2701\n",
      "Episode 416000, Loss: 0.0000, Reward: 0.2197, Avg Reward: 0.2783\n",
      "Episode 417000, Loss: -13.5212, Reward: 0.5295, Avg Reward: 0.2691\n",
      "Episode 418000, Loss: -18.1981, Reward: 0.6492, Avg Reward: 0.2674\n",
      "Episode 419000, Loss: -0.0001, Reward: 0.0102, Avg Reward: 0.2781\n",
      "Episode 420000, Loss: -14.3178, Reward: 0.5285, Avg Reward: 0.2794\n",
      "Episode 421000, Loss: -1.6930, Reward: 0.2996, Avg Reward: 0.2748\n",
      "Episode 422000, Loss: -0.0000, Reward: 0.1156, Avg Reward: 0.2688\n",
      "Episode 423000, Loss: 9.4605, Reward: 0.0327, Avg Reward: 0.2609\n",
      "Episode 424000, Loss: 0.0000, Reward: 0.2001, Avg Reward: 0.2735\n",
      "Episode 425000, Loss: 5.0867, Reward: 0.1291, Avg Reward: 0.2809\n",
      "Episode 426000, Loss: 0.3573, Reward: 0.4948, Avg Reward: 0.2680\n",
      "Episode 427000, Loss: 1.4369, Reward: 0.1586, Avg Reward: 0.2688\n",
      "Episode 428000, Loss: -0.0582, Reward: 0.0198, Avg Reward: 0.2719\n",
      "Episode 429000, Loss: -0.0032, Reward: 0.0564, Avg Reward: 0.2700\n",
      "Episode 430000, Loss: -28.1609, Reward: 0.7377, Avg Reward: 0.2546\n",
      "Episode 431000, Loss: 5.6718, Reward: 0.0916, Avg Reward: 0.2594\n",
      "Episode 432000, Loss: -0.0000, Reward: 0.0710, Avg Reward: 0.2515\n",
      "Episode 433000, Loss: 0.0000, Reward: 0.1958, Avg Reward: 0.2655\n",
      "Episode 434000, Loss: -3.5548, Reward: 0.4988, Avg Reward: 0.2587\n",
      "Episode 435000, Loss: -0.0029, Reward: 0.0106, Avg Reward: 0.2561\n",
      "Episode 436000, Loss: -0.2797, Reward: 0.1706, Avg Reward: 0.2694\n",
      "Episode 437000, Loss: -0.0003, Reward: 0.2073, Avg Reward: 0.2596\n",
      "Episode 438000, Loss: -0.0000, Reward: 0.2264, Avg Reward: 0.2739\n",
      "Episode 439000, Loss: -2.2421, Reward: 0.3077, Avg Reward: 0.2569\n",
      "Episode 440000, Loss: 0.0000, Reward: 0.3981, Avg Reward: 0.2707\n",
      "Episode 441000, Loss: 0.2714, Reward: 0.2953, Avg Reward: 0.2891\n",
      "Episode 442000, Loss: 1.2406, Reward: 0.0220, Avg Reward: 0.2775\n",
      "Episode 443000, Loss: -0.0043, Reward: 0.2425, Avg Reward: 0.2736\n",
      "Episode 444000, Loss: -2.1526, Reward: 0.3057, Avg Reward: 0.2670\n",
      "Episode 445000, Loss: 1.9886, Reward: 0.0308, Avg Reward: 0.2782\n",
      "Episode 446000, Loss: -0.0000, Reward: 0.3002, Avg Reward: 0.2849\n",
      "Episode 447000, Loss: -0.0000, Reward: 0.3693, Avg Reward: 0.2669\n",
      "Episode 448000, Loss: 0.0028, Reward: 0.3091, Avg Reward: 0.2703\n",
      "Episode 449000, Loss: -4.7784, Reward: 0.3672, Avg Reward: 0.2762\n",
      "Episode 450000, Loss: -2.6064, Reward: 0.3354, Avg Reward: 0.2744\n",
      "Episode 451000, Loss: -0.0158, Reward: 0.0303, Avg Reward: 0.2856\n",
      "Episode 452000, Loss: 1.4394, Reward: 0.2067, Avg Reward: 0.2815\n",
      "Episode 453000, Loss: -1.4729, Reward: 0.3250, Avg Reward: 0.2796\n",
      "Episode 454000, Loss: -1.6891, Reward: 0.5651, Avg Reward: 0.2736\n",
      "Episode 455000, Loss: -1.0082, Reward: 0.2856, Avg Reward: 0.2678\n",
      "Episode 456000, Loss: 0.0000, Reward: 0.0515, Avg Reward: 0.2752\n",
      "Episode 457000, Loss: 0.0000, Reward: 0.2306, Avg Reward: 0.2805\n",
      "Episode 458000, Loss: 0.0000, Reward: 0.1802, Avg Reward: 0.2705\n",
      "Episode 459000, Loss: 2.5970, Reward: 0.2226, Avg Reward: 0.2746\n",
      "Episode 460000, Loss: 0.0000, Reward: 0.1251, Avg Reward: 0.2727\n",
      "Episode 461000, Loss: -0.9284, Reward: 0.3221, Avg Reward: 0.2813\n",
      "Episode 462000, Loss: -11.1236, Reward: 0.4794, Avg Reward: 0.2848\n",
      "Episode 463000, Loss: 0.0000, Reward: 0.0975, Avg Reward: 0.2836\n",
      "Episode 464000, Loss: 0.0000, Reward: 0.0287, Avg Reward: 0.2706\n",
      "Episode 465000, Loss: -0.0000, Reward: 0.3532, Avg Reward: 0.2706\n",
      "Episode 466000, Loss: 2.6706, Reward: 0.2128, Avg Reward: 0.2718\n",
      "Episode 467000, Loss: -0.0000, Reward: 0.4247, Avg Reward: 0.2800\n",
      "Episode 468000, Loss: -31.7282, Reward: 0.7898, Avg Reward: 0.2692\n",
      "Episode 469000, Loss: -7.4441, Reward: 0.3871, Avg Reward: 0.2741\n",
      "Episode 470000, Loss: -14.0748, Reward: 0.4807, Avg Reward: 0.2687\n",
      "Episode 471000, Loss: 0.0000, Reward: 0.1445, Avg Reward: 0.2673\n",
      "Episode 472000, Loss: -0.0000, Reward: 0.3431, Avg Reward: 0.2615\n",
      "Episode 473000, Loss: 0.0000, Reward: 0.2422, Avg Reward: 0.2638\n",
      "Episode 474000, Loss: 0.0000, Reward: 0.1424, Avg Reward: 0.2634\n",
      "Episode 475000, Loss: 0.0000, Reward: 0.0631, Avg Reward: 0.2665\n",
      "Episode 476000, Loss: -14.8092, Reward: 0.5152, Avg Reward: 0.2716\n",
      "Episode 477000, Loss: 0.0000, Reward: 0.1644, Avg Reward: 0.2577\n",
      "Episode 478000, Loss: 0.0000, Reward: 0.2682, Avg Reward: 0.2698\n",
      "Episode 479000, Loss: -30.1925, Reward: 0.7268, Avg Reward: 0.2584\n",
      "Episode 480000, Loss: 0.0000, Reward: 0.2369, Avg Reward: 0.2669\n",
      "Episode 481000, Loss: -0.0000, Reward: 0.1884, Avg Reward: 0.2668\n",
      "Episode 482000, Loss: 0.0000, Reward: 0.1432, Avg Reward: 0.2636\n",
      "Episode 483000, Loss: -0.0048, Reward: 0.1612, Avg Reward: 0.2756\n",
      "Episode 484000, Loss: -0.0075, Reward: 0.0823, Avg Reward: 0.2721\n",
      "Episode 485000, Loss: 4.4085, Reward: 0.0778, Avg Reward: 0.2749\n",
      "Episode 486000, Loss: 0.4782, Reward: 0.2131, Avg Reward: 0.2832\n",
      "Episode 487000, Loss: -1.3219, Reward: 0.3428, Avg Reward: 0.2771\n",
      "Episode 488000, Loss: 1.1694, Reward: 0.1171, Avg Reward: 0.2821\n",
      "Episode 489000, Loss: -0.0000, Reward: 0.3175, Avg Reward: 0.2764\n",
      "Episode 490000, Loss: -0.0245, Reward: 0.1657, Avg Reward: 0.2854\n",
      "Episode 491000, Loss: 0.0000, Reward: 0.0826, Avg Reward: 0.2709\n",
      "Episode 492000, Loss: -0.0003, Reward: 0.2700, Avg Reward: 0.2747\n",
      "Episode 493000, Loss: 0.0000, Reward: 0.1240, Avg Reward: 0.2722\n",
      "Episode 494000, Loss: 0.0000, Reward: 0.0064, Avg Reward: 0.2769\n",
      "Episode 495000, Loss: 6.7989, Reward: 0.0930, Avg Reward: 0.2806\n",
      "Episode 496000, Loss: 0.0000, Reward: 0.1238, Avg Reward: 0.2792\n",
      "Episode 497000, Loss: 0.0000, Reward: 0.2825, Avg Reward: 0.2706\n",
      "Episode 498000, Loss: -0.0000, Reward: 0.4129, Avg Reward: 0.2723\n",
      "Episode 499000, Loss: -8.9834, Reward: 0.4373, Avg Reward: 0.2828\n",
      "Episode 500000, Loss: -12.4494, Reward: 0.4512, Avg Reward: 0.2665\n",
      "Episode 501000, Loss: -14.2341, Reward: 0.4664, Avg Reward: 0.2669\n",
      "Episode 502000, Loss: 0.0000, Reward: 0.0530, Avg Reward: 0.2754\n",
      "Episode 503000, Loss: 3.2501, Reward: 0.1383, Avg Reward: 0.2602\n",
      "Episode 504000, Loss: -0.0018, Reward: 0.1326, Avg Reward: 0.2694\n",
      "Episode 505000, Loss: -11.6401, Reward: 0.5016, Avg Reward: 0.2775\n",
      "Episode 506000, Loss: -0.0001, Reward: 0.2500, Avg Reward: 0.2919\n",
      "Episode 507000, Loss: 2.4757, Reward: 0.1847, Avg Reward: 0.2692\n",
      "Episode 508000, Loss: -4.4234, Reward: 0.3542, Avg Reward: 0.2868\n",
      "Episode 509000, Loss: -6.1930, Reward: 0.4735, Avg Reward: 0.2878\n",
      "Episode 510000, Loss: 1.0593, Reward: 0.2654, Avg Reward: 0.3005\n",
      "Episode 511000, Loss: -14.4142, Reward: 0.4991, Avg Reward: 0.2823\n",
      "Episode 512000, Loss: -0.0366, Reward: 0.2700, Avg Reward: 0.2676\n",
      "Episode 513000, Loss: -14.9158, Reward: 0.6002, Avg Reward: 0.2728\n",
      "Episode 514000, Loss: 0.0000, Reward: 0.1415, Avg Reward: 0.2764\n",
      "Episode 515000, Loss: -8.5279, Reward: 0.3953, Avg Reward: 0.2737\n",
      "Episode 516000, Loss: -0.0000, Reward: 0.0687, Avg Reward: 0.2608\n",
      "Episode 517000, Loss: 0.6514, Reward: 0.2833, Avg Reward: 0.2857\n",
      "Episode 518000, Loss: -6.0683, Reward: 0.3787, Avg Reward: 0.2774\n",
      "Episode 519000, Loss: -18.4670, Reward: 0.5419, Avg Reward: 0.2656\n",
      "Episode 520000, Loss: -0.0003, Reward: 0.1441, Avg Reward: 0.2646\n",
      "Episode 521000, Loss: -25.4217, Reward: 0.6993, Avg Reward: 0.2823\n",
      "Episode 522000, Loss: -2.7920, Reward: 0.3217, Avg Reward: 0.2802\n",
      "Episode 523000, Loss: -0.0027, Reward: 0.2726, Avg Reward: 0.2821\n",
      "Episode 524000, Loss: -0.0001, Reward: 0.2281, Avg Reward: 0.2922\n",
      "Episode 525000, Loss: -22.7334, Reward: 0.6507, Avg Reward: 0.2899\n",
      "Episode 526000, Loss: -7.8933, Reward: 0.4004, Avg Reward: 0.2781\n",
      "Episode 527000, Loss: -12.7647, Reward: 0.4844, Avg Reward: 0.2770\n",
      "Episode 528000, Loss: -15.0998, Reward: 0.5581, Avg Reward: 0.2817\n",
      "Episode 529000, Loss: -0.1799, Reward: 0.1852, Avg Reward: 0.2750\n",
      "Episode 530000, Loss: -2.3002, Reward: 0.3757, Avg Reward: 0.2891\n",
      "Episode 531000, Loss: 0.0000, Reward: 0.4302, Avg Reward: 0.2780\n",
      "Episode 532000, Loss: -0.0040, Reward: 0.0561, Avg Reward: 0.2869\n",
      "Episode 533000, Loss: -0.0000, Reward: 0.0885, Avg Reward: 0.2730\n",
      "Episode 534000, Loss: -7.0248, Reward: 0.4581, Avg Reward: 0.2722\n",
      "Episode 535000, Loss: -0.0411, Reward: 0.0469, Avg Reward: 0.2782\n",
      "Episode 536000, Loss: 2.2842, Reward: 0.1063, Avg Reward: 0.2907\n",
      "Episode 537000, Loss: -0.0223, Reward: 0.1338, Avg Reward: 0.2802\n",
      "Episode 538000, Loss: -0.0216, Reward: 0.1851, Avg Reward: 0.2835\n",
      "Episode 539000, Loss: 0.2483, Reward: 0.3904, Avg Reward: 0.2771\n",
      "Episode 540000, Loss: -9.4452, Reward: 0.4456, Avg Reward: 0.2860\n",
      "Episode 541000, Loss: -10.2763, Reward: 0.4740, Avg Reward: 0.2826\n",
      "Episode 542000, Loss: 0.3590, Reward: 0.5346, Avg Reward: 0.2729\n",
      "Episode 543000, Loss: 2.7059, Reward: 0.2230, Avg Reward: 0.2689\n",
      "Episode 544000, Loss: 2.1809, Reward: 0.2442, Avg Reward: 0.2833\n",
      "Episode 545000, Loss: -25.6936, Reward: 0.7072, Avg Reward: 0.2693\n",
      "Episode 546000, Loss: -0.0459, Reward: 0.0918, Avg Reward: 0.2829\n",
      "Episode 547000, Loss: 0.5877, Reward: 0.1205, Avg Reward: 0.2849\n",
      "Episode 548000, Loss: 0.0037, Reward: 0.2750, Avg Reward: 0.2840\n",
      "Episode 549000, Loss: 0.0000, Reward: 0.2010, Avg Reward: 0.2770\n",
      "Episode 550000, Loss: 0.3419, Reward: 0.2571, Avg Reward: 0.2808\n",
      "Episode 551000, Loss: -6.0554, Reward: 0.3719, Avg Reward: 0.2825\n",
      "Episode 552000, Loss: 0.1632, Reward: 0.2013, Avg Reward: 0.2851\n",
      "Episode 553000, Loss: -9.0318, Reward: 0.4310, Avg Reward: 0.2857\n",
      "Episode 554000, Loss: -6.6417, Reward: 0.4005, Avg Reward: 0.2846\n",
      "Episode 555000, Loss: -2.7001, Reward: 0.3567, Avg Reward: 0.3003\n",
      "Episode 556000, Loss: -0.0000, Reward: 0.2177, Avg Reward: 0.2930\n",
      "Episode 557000, Loss: -0.0010, Reward: 0.2643, Avg Reward: 0.2816\n",
      "Episode 558000, Loss: -0.0173, Reward: 0.0307, Avg Reward: 0.2845\n",
      "Episode 559000, Loss: -0.0000, Reward: 0.2473, Avg Reward: 0.2877\n",
      "Episode 560000, Loss: -0.0000, Reward: 0.4808, Avg Reward: 0.2824\n",
      "Episode 561000, Loss: 0.0000, Reward: 0.0123, Avg Reward: 0.2835\n",
      "Episode 562000, Loss: -3.0206, Reward: 0.3247, Avg Reward: 0.2740\n",
      "Episode 563000, Loss: -20.3004, Reward: 0.6234, Avg Reward: 0.2867\n",
      "Episode 564000, Loss: 0.0000, Reward: 0.0639, Avg Reward: 0.2810\n",
      "Episode 565000, Loss: -0.0030, Reward: 0.2486, Avg Reward: 0.2844\n",
      "Episode 566000, Loss: -6.3676, Reward: 0.4035, Avg Reward: 0.2742\n",
      "Episode 567000, Loss: -0.3137, Reward: 0.1410, Avg Reward: 0.2854\n",
      "Episode 568000, Loss: -0.0000, Reward: 0.0280, Avg Reward: 0.2879\n",
      "Episode 569000, Loss: -8.3396, Reward: 0.4461, Avg Reward: 0.2829\n",
      "Episode 570000, Loss: 3.5565, Reward: 0.0879, Avg Reward: 0.2872\n",
      "Episode 571000, Loss: 1.1441, Reward: 0.2270, Avg Reward: 0.2829\n",
      "Episode 572000, Loss: -13.8743, Reward: 0.5326, Avg Reward: 0.2814\n",
      "Episode 573000, Loss: 0.0000, Reward: 0.2017, Avg Reward: 0.2688\n",
      "Episode 574000, Loss: 0.0000, Reward: 0.1328, Avg Reward: 0.2733\n",
      "Episode 575000, Loss: 0.0000, Reward: 0.2254, Avg Reward: 0.2648\n",
      "Episode 576000, Loss: -0.7235, Reward: 0.3060, Avg Reward: 0.2680\n",
      "Episode 577000, Loss: -10.1918, Reward: 0.4381, Avg Reward: 0.2793\n",
      "Episode 578000, Loss: -0.0002, Reward: 0.2464, Avg Reward: 0.2812\n",
      "Episode 579000, Loss: 0.0000, Reward: 0.3688, Avg Reward: 0.2888\n",
      "Episode 580000, Loss: 0.0000, Reward: 0.1754, Avg Reward: 0.2704\n",
      "Episode 581000, Loss: 0.0000, Reward: 0.1759, Avg Reward: 0.2811\n",
      "Episode 582000, Loss: 0.1081, Reward: 0.2679, Avg Reward: 0.2698\n",
      "Episode 583000, Loss: 0.0000, Reward: 0.1984, Avg Reward: 0.2745\n",
      "Episode 584000, Loss: -0.0000, Reward: 0.4438, Avg Reward: 0.2781\n",
      "Episode 585000, Loss: -16.0003, Reward: 0.5571, Avg Reward: 0.2842\n",
      "Episode 586000, Loss: -0.0011, Reward: 0.2251, Avg Reward: 0.2848\n",
      "Episode 587000, Loss: -0.0026, Reward: 0.1964, Avg Reward: 0.2808\n",
      "Episode 588000, Loss: -15.6607, Reward: 0.5100, Avg Reward: 0.2832\n",
      "Episode 589000, Loss: -3.0378, Reward: 0.3585, Avg Reward: 0.3038\n",
      "Episode 590000, Loss: 0.0000, Reward: 0.4036, Avg Reward: 0.2970\n",
      "Episode 591000, Loss: -15.9196, Reward: 0.5106, Avg Reward: 0.2918\n",
      "Episode 592000, Loss: 2.9180, Reward: 0.1428, Avg Reward: 0.2875\n",
      "Episode 593000, Loss: 2.8114, Reward: 0.1827, Avg Reward: 0.2801\n",
      "Episode 594000, Loss: -17.0879, Reward: 0.5367, Avg Reward: 0.2842\n",
      "Episode 595000, Loss: 6.2447, Reward: 0.0240, Avg Reward: 0.2929\n",
      "Episode 596000, Loss: -0.5755, Reward: 0.3225, Avg Reward: 0.3025\n",
      "Episode 597000, Loss: -0.1259, Reward: 0.3068, Avg Reward: 0.2913\n",
      "Episode 598000, Loss: -17.1500, Reward: 0.5483, Avg Reward: 0.2858\n",
      "Episode 599000, Loss: 3.4766, Reward: 0.2264, Avg Reward: 0.2881\n",
      "Episode 600000, Loss: -0.0000, Reward: 0.4364, Avg Reward: 0.2789\n",
      "Episode 601000, Loss: 1.0284, Reward: 0.3031, Avg Reward: 0.2994\n",
      "Episode 602000, Loss: -0.0025, Reward: 0.0250, Avg Reward: 0.2929\n",
      "Episode 603000, Loss: -1.2335, Reward: 0.3925, Avg Reward: 0.2741\n",
      "Episode 604000, Loss: 0.7727, Reward: 0.1000, Avg Reward: 0.2943\n",
      "Episode 605000, Loss: -0.0000, Reward: 0.0937, Avg Reward: 0.2787\n",
      "Episode 606000, Loss: -0.0000, Reward: 0.3226, Avg Reward: 0.2848\n",
      "Episode 607000, Loss: 0.0000, Reward: 0.1535, Avg Reward: 0.2958\n",
      "Episode 608000, Loss: -31.4452, Reward: 0.7742, Avg Reward: 0.2920\n",
      "Episode 609000, Loss: 0.0000, Reward: 0.5429, Avg Reward: 0.2804\n",
      "Episode 610000, Loss: -0.0000, Reward: 0.1902, Avg Reward: 0.2755\n",
      "Episode 611000, Loss: -6.4374, Reward: 0.3967, Avg Reward: 0.2857\n",
      "Episode 612000, Loss: -0.0091, Reward: 0.2219, Avg Reward: 0.2845\n",
      "Episode 613000, Loss: -6.1113, Reward: 0.4799, Avg Reward: 0.2990\n",
      "Episode 614000, Loss: 2.7984, Reward: 0.2087, Avg Reward: 0.2760\n",
      "Episode 615000, Loss: -12.1891, Reward: 0.5718, Avg Reward: 0.2905\n",
      "Episode 616000, Loss: -0.0000, Reward: 0.4811, Avg Reward: 0.2810\n",
      "Episode 617000, Loss: -6.4302, Reward: 0.4239, Avg Reward: 0.2952\n",
      "Episode 618000, Loss: 0.0000, Reward: 0.5309, Avg Reward: 0.2871\n",
      "Episode 619000, Loss: -0.0001, Reward: 0.2082, Avg Reward: 0.2833\n",
      "Episode 620000, Loss: -0.0000, Reward: 0.1889, Avg Reward: 0.2817\n",
      "Episode 621000, Loss: 4.1191, Reward: 0.1002, Avg Reward: 0.2911\n",
      "Episode 622000, Loss: 1.8944, Reward: 0.1672, Avg Reward: 0.2793\n",
      "Episode 623000, Loss: -0.0000, Reward: 0.2422, Avg Reward: 0.2752\n",
      "Episode 624000, Loss: 2.1213, Reward: 0.1998, Avg Reward: 0.2807\n",
      "Episode 625000, Loss: -12.2730, Reward: 0.4909, Avg Reward: 0.2898\n",
      "Episode 626000, Loss: 0.0000, Reward: 0.1922, Avg Reward: 0.2844\n",
      "Episode 627000, Loss: -0.0000, Reward: 0.0566, Avg Reward: 0.2838\n",
      "Episode 628000, Loss: 0.0000, Reward: 0.0987, Avg Reward: 0.2806\n",
      "Episode 629000, Loss: -0.0342, Reward: 0.2986, Avg Reward: 0.2885\n",
      "Episode 630000, Loss: 0.0000, Reward: 0.2089, Avg Reward: 0.2867\n",
      "Episode 631000, Loss: -4.3425, Reward: 0.3558, Avg Reward: 0.2920\n",
      "Episode 632000, Loss: -27.5592, Reward: 0.7145, Avg Reward: 0.2817\n",
      "Episode 633000, Loss: -2.3737, Reward: 0.3938, Avg Reward: 0.2783\n",
      "Episode 634000, Loss: -8.9668, Reward: 0.4700, Avg Reward: 0.2876\n",
      "Episode 635000, Loss: -2.9943, Reward: 0.3966, Avg Reward: 0.2795\n",
      "Episode 636000, Loss: -0.0002, Reward: 0.0243, Avg Reward: 0.2850\n",
      "Episode 637000, Loss: -0.0000, Reward: 0.4888, Avg Reward: 0.2811\n",
      "Episode 638000, Loss: 4.7757, Reward: 0.1068, Avg Reward: 0.2922\n",
      "Episode 639000, Loss: 6.2116, Reward: 0.0928, Avg Reward: 0.3036\n",
      "Episode 640000, Loss: -16.0212, Reward: 0.5485, Avg Reward: 0.2981\n",
      "Episode 641000, Loss: -0.3609, Reward: 0.3259, Avg Reward: 0.3088\n",
      "Episode 642000, Loss: -0.0000, Reward: 0.3496, Avg Reward: 0.3015\n",
      "Episode 643000, Loss: -6.8510, Reward: 0.3883, Avg Reward: 0.2966\n",
      "Episode 644000, Loss: 1.9518, Reward: 0.1817, Avg Reward: 0.2910\n",
      "Episode 645000, Loss: -0.0000, Reward: 0.0066, Avg Reward: 0.3010\n",
      "Episode 646000, Loss: -0.0001, Reward: 0.2561, Avg Reward: 0.2870\n",
      "Episode 647000, Loss: -3.4118, Reward: 0.3436, Avg Reward: 0.2855\n",
      "Episode 648000, Loss: 0.0000, Reward: 0.0991, Avg Reward: 0.3049\n",
      "Episode 649000, Loss: -0.0008, Reward: 0.0640, Avg Reward: 0.2900\n",
      "Episode 650000, Loss: 0.0032, Reward: 0.5391, Avg Reward: 0.2921\n",
      "Episode 651000, Loss: -0.0000, Reward: 0.1846, Avg Reward: 0.2955\n",
      "Episode 652000, Loss: 0.0000, Reward: 0.2083, Avg Reward: 0.2799\n",
      "Episode 653000, Loss: -38.8292, Reward: 0.8911, Avg Reward: 0.2846\n",
      "Episode 654000, Loss: 0.0000, Reward: 0.2031, Avg Reward: 0.2932\n",
      "Episode 655000, Loss: -0.0000, Reward: 0.1787, Avg Reward: 0.2913\n",
      "Episode 656000, Loss: -16.1316, Reward: 0.5614, Avg Reward: 0.2845\n",
      "Episode 657000, Loss: -19.7875, Reward: 0.5751, Avg Reward: 0.2880\n",
      "Episode 658000, Loss: -12.5701, Reward: 0.4812, Avg Reward: 0.2876\n",
      "Episode 659000, Loss: 3.4695, Reward: 0.2263, Avg Reward: 0.2819\n",
      "Episode 660000, Loss: 0.0000, Reward: 0.5354, Avg Reward: 0.2888\n",
      "Episode 661000, Loss: -6.7296, Reward: 0.3918, Avg Reward: 0.2850\n",
      "Episode 662000, Loss: -19.3672, Reward: 0.5699, Avg Reward: 0.2937\n",
      "Episode 663000, Loss: -6.4759, Reward: 0.3961, Avg Reward: 0.2892\n",
      "Episode 664000, Loss: -0.0000, Reward: 0.1252, Avg Reward: 0.2907\n",
      "Episode 665000, Loss: -0.0000, Reward: 0.0024, Avg Reward: 0.3031\n",
      "Episode 666000, Loss: 0.0000, Reward: 0.0484, Avg Reward: 0.2967\n",
      "Episode 667000, Loss: -0.0000, Reward: 0.2178, Avg Reward: 0.2875\n",
      "Episode 668000, Loss: -18.5897, Reward: 0.5435, Avg Reward: 0.2754\n",
      "Episode 669000, Loss: 0.0000, Reward: 0.1718, Avg Reward: 0.2851\n",
      "Episode 670000, Loss: -0.0000, Reward: 0.1431, Avg Reward: 0.2919\n",
      "Episode 671000, Loss: -9.2873, Reward: 0.4270, Avg Reward: 0.2871\n",
      "Episode 672000, Loss: -0.0000, Reward: 0.4493, Avg Reward: 0.2872\n",
      "Episode 673000, Loss: -1.4355, Reward: 0.3327, Avg Reward: 0.2820\n",
      "Episode 674000, Loss: -6.4755, Reward: 0.4079, Avg Reward: 0.2768\n",
      "Episode 675000, Loss: 0.0000, Reward: 0.0813, Avg Reward: 0.2897\n",
      "Episode 676000, Loss: 0.9540, Reward: 0.2763, Avg Reward: 0.2854\n",
      "Episode 677000, Loss: 0.0000, Reward: 0.1838, Avg Reward: 0.2820\n",
      "Episode 678000, Loss: 0.0000, Reward: 0.0898, Avg Reward: 0.2863\n",
      "Episode 679000, Loss: -0.6739, Reward: 0.3013, Avg Reward: 0.2790\n",
      "Episode 680000, Loss: -3.1180, Reward: 0.3554, Avg Reward: 0.2917\n",
      "Episode 681000, Loss: -0.0000, Reward: 0.1907, Avg Reward: 0.2869\n",
      "Episode 682000, Loss: -5.9615, Reward: 0.3920, Avg Reward: 0.2925\n",
      "Episode 683000, Loss: 0.0000, Reward: 0.0704, Avg Reward: 0.2915\n",
      "Episode 684000, Loss: 0.2419, Reward: 0.2698, Avg Reward: 0.2872\n",
      "Episode 685000, Loss: -6.7076, Reward: 0.5127, Avg Reward: 0.2799\n",
      "Episode 686000, Loss: -5.0305, Reward: 0.3501, Avg Reward: 0.2830\n",
      "Episode 687000, Loss: -0.1299, Reward: 0.2906, Avg Reward: 0.2828\n",
      "Episode 688000, Loss: 0.2708, Reward: 0.2552, Avg Reward: 0.2856\n",
      "Episode 689000, Loss: 0.0000, Reward: 0.0460, Avg Reward: 0.2861\n",
      "Episode 690000, Loss: -5.8303, Reward: 0.3816, Avg Reward: 0.2865\n",
      "Episode 691000, Loss: -2.9944, Reward: 0.3428, Avg Reward: 0.2983\n",
      "Episode 692000, Loss: -0.0343, Reward: 0.1808, Avg Reward: 0.2881\n",
      "Episode 693000, Loss: 0.0000, Reward: 0.0850, Avg Reward: 0.2942\n",
      "Episode 694000, Loss: -10.7188, Reward: 0.4641, Avg Reward: 0.2897\n",
      "Episode 695000, Loss: 0.6982, Reward: 0.2300, Avg Reward: 0.3078\n",
      "Episode 696000, Loss: 0.2556, Reward: 0.2741, Avg Reward: 0.2984\n",
      "Episode 697000, Loss: 2.1487, Reward: 0.1622, Avg Reward: 0.2965\n",
      "Episode 698000, Loss: 2.5028, Reward: 0.1804, Avg Reward: 0.3031\n",
      "Episode 699000, Loss: 0.0000, Reward: 0.2453, Avg Reward: 0.2908\n",
      "Episode 700000, Loss: -11.8835, Reward: 0.5332, Avg Reward: 0.3094\n",
      "Episode 701000, Loss: 0.0000, Reward: 0.0466, Avg Reward: 0.2922\n",
      "Episode 702000, Loss: 0.9739, Reward: 0.1261, Avg Reward: 0.2836\n",
      "Episode 703000, Loss: -0.0000, Reward: 0.1454, Avg Reward: 0.2893\n",
      "Episode 704000, Loss: 0.0000, Reward: 0.2657, Avg Reward: 0.2823\n",
      "Episode 705000, Loss: -0.0057, Reward: 0.2155, Avg Reward: 0.2962\n",
      "Episode 706000, Loss: -0.0000, Reward: 0.4356, Avg Reward: 0.2836\n",
      "Episode 707000, Loss: 0.0000, Reward: 0.1268, Avg Reward: 0.2981\n",
      "Episode 708000, Loss: 3.4245, Reward: 0.2161, Avg Reward: 0.2961\n",
      "Episode 709000, Loss: 4.1185, Reward: 0.2468, Avg Reward: 0.2913\n",
      "Episode 710000, Loss: -19.1656, Reward: 0.6347, Avg Reward: 0.2982\n",
      "Episode 711000, Loss: -0.3162, Reward: 0.3005, Avg Reward: 0.2981\n",
      "Episode 712000, Loss: -1.5818, Reward: 0.3325, Avg Reward: 0.2857\n",
      "Episode 713000, Loss: -1.7736, Reward: 0.3258, Avg Reward: 0.2957\n",
      "Episode 714000, Loss: -24.0987, Reward: 0.6584, Avg Reward: 0.2958\n",
      "Episode 715000, Loss: 1.3597, Reward: 0.2222, Avg Reward: 0.2965\n",
      "Episode 716000, Loss: 0.0000, Reward: 0.0892, Avg Reward: 0.2906\n",
      "Episode 717000, Loss: -17.3626, Reward: 0.5928, Avg Reward: 0.2947\n",
      "Episode 718000, Loss: -0.0000, Reward: 0.0941, Avg Reward: 0.3042\n",
      "Episode 719000, Loss: -0.0000, Reward: 0.0698, Avg Reward: 0.2954\n",
      "Episode 720000, Loss: -4.2940, Reward: 0.3803, Avg Reward: 0.2933\n",
      "Episode 721000, Loss: 3.8470, Reward: 0.2335, Avg Reward: 0.2916\n",
      "Episode 722000, Loss: -7.5782, Reward: 0.3931, Avg Reward: 0.2902\n",
      "Episode 723000, Loss: 0.2627, Reward: 0.2797, Avg Reward: 0.3017\n",
      "Episode 724000, Loss: 0.5014, Reward: 0.1865, Avg Reward: 0.2839\n",
      "Episode 725000, Loss: -2.9066, Reward: 0.4585, Avg Reward: 0.2822\n",
      "Episode 726000, Loss: 0.0000, Reward: 0.0075, Avg Reward: 0.2880\n",
      "Episode 727000, Loss: -0.0023, Reward: 0.0992, Avg Reward: 0.2875\n",
      "Episode 728000, Loss: -29.2612, Reward: 0.7131, Avg Reward: 0.2802\n",
      "Episode 729000, Loss: 0.0000, Reward: 0.1380, Avg Reward: 0.2900\n",
      "Episode 730000, Loss: -0.2185, Reward: 0.0070, Avg Reward: 0.3010\n",
      "Episode 731000, Loss: 0.2502, Reward: 0.2094, Avg Reward: 0.2860\n",
      "Episode 732000, Loss: -23.7015, Reward: 0.6514, Avg Reward: 0.2865\n",
      "Episode 733000, Loss: -0.0066, Reward: 0.0851, Avg Reward: 0.2854\n",
      "Episode 734000, Loss: -0.0261, Reward: 0.0600, Avg Reward: 0.2915\n",
      "Episode 735000, Loss: -0.0023, Reward: 0.1435, Avg Reward: 0.2893\n",
      "Episode 736000, Loss: -0.0000, Reward: 0.2210, Avg Reward: 0.2932\n",
      "Episode 737000, Loss: 0.0000, Reward: 0.1602, Avg Reward: 0.2921\n",
      "Episode 738000, Loss: 0.0000, Reward: 0.1697, Avg Reward: 0.2825\n",
      "Episode 739000, Loss: -0.0428, Reward: 0.0250, Avg Reward: 0.2860\n",
      "Episode 740000, Loss: 0.0004, Reward: 0.2819, Avg Reward: 0.2779\n",
      "Episode 741000, Loss: 0.0000, Reward: 0.1392, Avg Reward: 0.2791\n",
      "Episode 742000, Loss: -6.2616, Reward: 0.3761, Avg Reward: 0.2950\n",
      "Episode 743000, Loss: -2.1557, Reward: 0.3126, Avg Reward: 0.2883\n",
      "Episode 744000, Loss: -2.1228, Reward: 0.3168, Avg Reward: 0.2864\n",
      "Episode 745000, Loss: 0.0000, Reward: 0.0142, Avg Reward: 0.2888\n",
      "Episode 746000, Loss: -0.0000, Reward: 0.0735, Avg Reward: 0.2917\n",
      "Episode 747000, Loss: -9.2035, Reward: 0.4043, Avg Reward: 0.2942\n",
      "Episode 748000, Loss: -0.0142, Reward: 0.2944, Avg Reward: 0.2875\n",
      "Episode 749000, Loss: 0.0000, Reward: 0.2277, Avg Reward: 0.2912\n",
      "Episode 750000, Loss: -0.0000, Reward: 0.3078, Avg Reward: 0.2900\n",
      "Episode 751000, Loss: 2.1523, Reward: 0.2446, Avg Reward: 0.2764\n",
      "Episode 752000, Loss: -0.0000, Reward: 0.3580, Avg Reward: 0.2914\n",
      "Episode 753000, Loss: -11.2958, Reward: 0.4709, Avg Reward: 0.3007\n",
      "Episode 754000, Loss: 10.1710, Reward: 0.0418, Avg Reward: 0.2917\n",
      "Episode 755000, Loss: 4.4473, Reward: 0.1259, Avg Reward: 0.2823\n",
      "Episode 756000, Loss: 0.0000, Reward: 0.1316, Avg Reward: 0.2922\n",
      "Episode 757000, Loss: -5.3704, Reward: 0.3535, Avg Reward: 0.2773\n",
      "Episode 758000, Loss: 0.0000, Reward: 0.0660, Avg Reward: 0.2906\n",
      "Episode 759000, Loss: 0.0000, Reward: 0.2657, Avg Reward: 0.2880\n",
      "Episode 760000, Loss: 0.2053, Reward: 0.2781, Avg Reward: 0.2871\n",
      "Episode 761000, Loss: 0.0000, Reward: 0.2319, Avg Reward: 0.2867\n",
      "Episode 762000, Loss: -1.5604, Reward: 0.3294, Avg Reward: 0.2916\n",
      "Episode 763000, Loss: -5.3953, Reward: 0.3819, Avg Reward: 0.2864\n",
      "Episode 764000, Loss: 0.0000, Reward: 0.1468, Avg Reward: 0.3045\n",
      "Episode 765000, Loss: 0.0000, Reward: 0.1517, Avg Reward: 0.2945\n",
      "Episode 766000, Loss: -8.0083, Reward: 0.4200, Avg Reward: 0.2846\n",
      "Episode 767000, Loss: -0.0030, Reward: 0.2365, Avg Reward: 0.2887\n",
      "Episode 768000, Loss: -0.3179, Reward: 0.2975, Avg Reward: 0.2901\n",
      "Episode 769000, Loss: -38.8064, Reward: 0.9407, Avg Reward: 0.2916\n",
      "Episode 770000, Loss: -0.0000, Reward: 0.1764, Avg Reward: 0.2857\n",
      "Episode 771000, Loss: -0.0000, Reward: 0.5152, Avg Reward: 0.2863\n",
      "Episode 772000, Loss: 1.4667, Reward: 0.1606, Avg Reward: 0.2870\n",
      "Episode 773000, Loss: -0.0095, Reward: 0.0256, Avg Reward: 0.2900\n",
      "Episode 774000, Loss: 0.0608, Reward: 0.5557, Avg Reward: 0.2878\n",
      "Episode 775000, Loss: -25.9948, Reward: 0.7056, Avg Reward: 0.2874\n",
      "Episode 776000, Loss: 0.0000, Reward: 0.1649, Avg Reward: 0.2903\n",
      "Episode 777000, Loss: -0.0002, Reward: 0.2200, Avg Reward: 0.2876\n",
      "Episode 778000, Loss: -1.2406, Reward: 0.2984, Avg Reward: 0.2815\n",
      "Episode 779000, Loss: -0.0574, Reward: 0.0145, Avg Reward: 0.2817\n",
      "Episode 780000, Loss: -0.0074, Reward: 0.0812, Avg Reward: 0.2782\n",
      "Episode 781000, Loss: 0.0000, Reward: 0.1130, Avg Reward: 0.2804\n",
      "Episode 782000, Loss: 0.0059, Reward: 0.4172, Avg Reward: 0.2837\n",
      "Episode 783000, Loss: -0.0004, Reward: 0.1457, Avg Reward: 0.2860\n",
      "Episode 784000, Loss: 0.0000, Reward: 0.2591, Avg Reward: 0.2825\n",
      "Episode 785000, Loss: -0.0479, Reward: 0.2855, Avg Reward: 0.2816\n",
      "Episode 786000, Loss: -19.8247, Reward: 0.6098, Avg Reward: 0.2756\n",
      "Episode 787000, Loss: -35.5308, Reward: 0.8467, Avg Reward: 0.2715\n",
      "Episode 788000, Loss: -26.5040, Reward: 0.6674, Avg Reward: 0.2932\n",
      "Episode 789000, Loss: -7.5433, Reward: 0.3964, Avg Reward: 0.2935\n",
      "Episode 790000, Loss: 0.0000, Reward: 0.1999, Avg Reward: 0.2731\n",
      "Episode 791000, Loss: 0.0000, Reward: 0.2091, Avg Reward: 0.2852\n",
      "Episode 792000, Loss: 0.0000, Reward: 0.0271, Avg Reward: 0.2818\n",
      "Episode 793000, Loss: 5.0053, Reward: 0.2025, Avg Reward: 0.2880\n",
      "Episode 794000, Loss: -0.0116, Reward: 0.2109, Avg Reward: 0.2756\n",
      "Episode 795000, Loss: -16.2867, Reward: 0.5314, Avg Reward: 0.2830\n",
      "Episode 796000, Loss: -0.0121, Reward: 0.1987, Avg Reward: 0.2911\n",
      "Episode 797000, Loss: -7.1820, Reward: 0.3967, Avg Reward: 0.2852\n",
      "Episode 798000, Loss: -17.3823, Reward: 0.5822, Avg Reward: 0.2913\n",
      "Episode 799000, Loss: 2.2119, Reward: 0.1050, Avg Reward: 0.2926\n",
      "Episode 800000, Loss: -13.9489, Reward: 0.4839, Avg Reward: 0.2791\n",
      "Episode 801000, Loss: -9.3046, Reward: 0.4295, Avg Reward: 0.2812\n",
      "Episode 802000, Loss: -3.9935, Reward: 0.3639, Avg Reward: 0.2865\n",
      "Episode 803000, Loss: -18.0848, Reward: 0.5448, Avg Reward: 0.2858\n",
      "Episode 804000, Loss: -0.0000, Reward: 0.0293, Avg Reward: 0.2839\n",
      "Episode 805000, Loss: -0.0000, Reward: 0.1158, Avg Reward: 0.2760\n",
      "Episode 806000, Loss: -1.1082, Reward: 0.0447, Avg Reward: 0.2832\n",
      "Episode 807000, Loss: -24.0507, Reward: 0.8424, Avg Reward: 0.2878\n",
      "Episode 808000, Loss: 0.0000, Reward: 0.0693, Avg Reward: 0.2845\n",
      "Episode 809000, Loss: 0.3517, Reward: 0.2700, Avg Reward: 0.2824\n",
      "Episode 810000, Loss: 2.3467, Reward: 0.0983, Avg Reward: 0.2857\n",
      "Episode 811000, Loss: -0.0000, Reward: 0.5025, Avg Reward: 0.2952\n",
      "Episode 812000, Loss: 2.2507, Reward: 0.1259, Avg Reward: 0.2809\n",
      "Episode 813000, Loss: -13.5717, Reward: 0.4787, Avg Reward: 0.2897\n",
      "Episode 814000, Loss: 0.3481, Reward: 0.0381, Avg Reward: 0.2833\n",
      "Episode 815000, Loss: 0.0000, Reward: 0.2574, Avg Reward: 0.2834\n",
      "Episode 816000, Loss: 0.0000, Reward: 0.0931, Avg Reward: 0.2830\n",
      "Episode 817000, Loss: 0.0004, Reward: 0.5095, Avg Reward: 0.2746\n",
      "Episode 818000, Loss: 7.6747, Reward: 0.1566, Avg Reward: 0.2746\n",
      "Episode 819000, Loss: 0.7406, Reward: 0.2749, Avg Reward: 0.2922\n",
      "Episode 820000, Loss: -16.5760, Reward: 0.5939, Avg Reward: 0.2871\n",
      "Episode 821000, Loss: 0.0000, Reward: 0.1840, Avg Reward: 0.2886\n",
      "Episode 822000, Loss: 1.6295, Reward: 0.2657, Avg Reward: 0.2744\n",
      "Episode 823000, Loss: -0.1608, Reward: 0.4086, Avg Reward: 0.2810\n",
      "Episode 824000, Loss: -6.1143, Reward: 0.4034, Avg Reward: 0.2989\n",
      "Episode 825000, Loss: 0.0000, Reward: 0.2165, Avg Reward: 0.2861\n",
      "Episode 826000, Loss: -19.3110, Reward: 0.5904, Avg Reward: 0.2899\n",
      "Episode 827000, Loss: 1.9875, Reward: 0.0570, Avg Reward: 0.2901\n",
      "Episode 828000, Loss: 0.0000, Reward: 0.0432, Avg Reward: 0.2724\n",
      "Episode 829000, Loss: 4.4515, Reward: 0.0020, Avg Reward: 0.2816\n",
      "Episode 830000, Loss: -3.4685, Reward: 0.3983, Avg Reward: 0.2896\n",
      "Episode 831000, Loss: 0.0000, Reward: 0.1922, Avg Reward: 0.2715\n",
      "Episode 832000, Loss: 0.0010, Reward: 0.3931, Avg Reward: 0.2887\n",
      "Episode 833000, Loss: -1.4714, Reward: 0.3289, Avg Reward: 0.2875\n",
      "Episode 834000, Loss: 7.3588, Reward: 0.1184, Avg Reward: 0.2688\n",
      "Episode 835000, Loss: 5.4865, Reward: 0.0806, Avg Reward: 0.2807\n",
      "Episode 836000, Loss: -0.0000, Reward: 0.3121, Avg Reward: 0.2617\n",
      "Episode 837000, Loss: -17.5481, Reward: 0.5504, Avg Reward: 0.2763\n",
      "Episode 838000, Loss: -9.9688, Reward: 0.4476, Avg Reward: 0.2814\n",
      "Episode 839000, Loss: -0.0000, Reward: 0.3595, Avg Reward: 0.2865\n",
      "Episode 840000, Loss: -3.7282, Reward: 0.3368, Avg Reward: 0.2824\n",
      "Episode 841000, Loss: 0.0000, Reward: 0.0846, Avg Reward: 0.2854\n",
      "Episode 842000, Loss: 13.3040, Reward: 0.0490, Avg Reward: 0.2791\n",
      "Episode 843000, Loss: -0.0000, Reward: 0.0862, Avg Reward: 0.2958\n",
      "Episode 844000, Loss: -0.0296, Reward: 0.1927, Avg Reward: 0.2753\n",
      "Episode 845000, Loss: -7.4553, Reward: 0.3974, Avg Reward: 0.2805\n",
      "Episode 846000, Loss: -0.0120, Reward: 0.1471, Avg Reward: 0.2748\n",
      "Episode 847000, Loss: 0.5726, Reward: 0.2447, Avg Reward: 0.2766\n",
      "Episode 848000, Loss: 7.3313, Reward: 0.1912, Avg Reward: 0.2722\n",
      "Episode 849000, Loss: 11.0578, Reward: 0.0607, Avg Reward: 0.2981\n",
      "Episode 850000, Loss: -0.1022, Reward: 0.1019, Avg Reward: 0.2781\n",
      "Episode 851000, Loss: -22.2816, Reward: 0.6074, Avg Reward: 0.2789\n",
      "Episode 852000, Loss: 2.3296, Reward: 0.0365, Avg Reward: 0.2766\n",
      "Episode 853000, Loss: 2.7629, Reward: 0.1627, Avg Reward: 0.2863\n",
      "Episode 854000, Loss: -0.0017, Reward: 0.0329, Avg Reward: 0.2769\n",
      "Episode 855000, Loss: 3.2263, Reward: 0.2239, Avg Reward: 0.2881\n",
      "Episode 856000, Loss: 0.0000, Reward: 0.3018, Avg Reward: 0.2798\n",
      "Episode 857000, Loss: 0.2213, Reward: 0.2564, Avg Reward: 0.2678\n",
      "Episode 858000, Loss: -0.6115, Reward: 0.2833, Avg Reward: 0.2794\n",
      "Episode 859000, Loss: 0.2569, Reward: 0.2753, Avg Reward: 0.2826\n",
      "Episode 860000, Loss: 4.6253, Reward: 0.1373, Avg Reward: 0.2984\n",
      "Episode 861000, Loss: 0.0000, Reward: 0.0052, Avg Reward: 0.2938\n",
      "Episode 862000, Loss: 8.2298, Reward: 0.1398, Avg Reward: 0.2890\n",
      "Episode 863000, Loss: -0.0000, Reward: 0.2355, Avg Reward: 0.2936\n",
      "Episode 864000, Loss: -0.5959, Reward: 0.3554, Avg Reward: 0.2957\n",
      "Episode 865000, Loss: -0.0000, Reward: 0.1004, Avg Reward: 0.2841\n",
      "Episode 866000, Loss: 0.0018, Reward: 0.3471, Avg Reward: 0.2875\n",
      "Episode 867000, Loss: -4.1734, Reward: 0.4824, Avg Reward: 0.2978\n",
      "Episode 868000, Loss: -10.3884, Reward: 0.4377, Avg Reward: 0.2933\n",
      "Episode 869000, Loss: -12.5671, Reward: 0.4788, Avg Reward: 0.2961\n",
      "Episode 870000, Loss: 0.0000, Reward: 0.0201, Avg Reward: 0.2815\n",
      "Episode 871000, Loss: 3.7739, Reward: 0.2253, Avg Reward: 0.2793\n",
      "Episode 872000, Loss: -12.4895, Reward: 0.4979, Avg Reward: 0.2890\n",
      "Episode 873000, Loss: -27.1852, Reward: 0.7835, Avg Reward: 0.2963\n",
      "Episode 874000, Loss: -0.0000, Reward: 0.3679, Avg Reward: 0.2873\n",
      "Episode 875000, Loss: -0.0000, Reward: 0.0345, Avg Reward: 0.2837\n",
      "Episode 876000, Loss: 3.6942, Reward: 0.1787, Avg Reward: 0.2881\n",
      "Episode 877000, Loss: -16.3599, Reward: 0.5462, Avg Reward: 0.2800\n",
      "Episode 878000, Loss: -40.2754, Reward: 0.8830, Avg Reward: 0.2887\n",
      "Episode 879000, Loss: -0.0033, Reward: 0.1244, Avg Reward: 0.2746\n",
      "Episode 880000, Loss: 0.2545, Reward: 0.2227, Avg Reward: 0.2843\n",
      "Episode 881000, Loss: 4.8329, Reward: 0.1755, Avg Reward: 0.2730\n",
      "Episode 882000, Loss: 0.9966, Reward: 0.2208, Avg Reward: 0.2627\n",
      "Episode 883000, Loss: -6.1438, Reward: 0.3785, Avg Reward: 0.2768\n",
      "Episode 884000, Loss: 4.0408, Reward: 0.0333, Avg Reward: 0.2549\n",
      "Episode 885000, Loss: 0.0000, Reward: 0.0323, Avg Reward: 0.2655\n",
      "Episode 886000, Loss: 3.0560, Reward: 0.1805, Avg Reward: 0.2729\n",
      "Episode 887000, Loss: -0.0000, Reward: 0.2818, Avg Reward: 0.2799\n",
      "Episode 888000, Loss: -9.7161, Reward: 0.4845, Avg Reward: 0.2722\n",
      "Episode 889000, Loss: -15.5159, Reward: 0.5144, Avg Reward: 0.2869\n",
      "Episode 890000, Loss: -0.0498, Reward: 0.2462, Avg Reward: 0.2942\n",
      "Episode 891000, Loss: 3.6990, Reward: 0.2029, Avg Reward: 0.2855\n",
      "Episode 892000, Loss: -0.0000, Reward: 0.2564, Avg Reward: 0.2838\n",
      "Episode 893000, Loss: -0.0005, Reward: 0.1726, Avg Reward: 0.2935\n",
      "Episode 894000, Loss: -0.0001, Reward: 0.0508, Avg Reward: 0.2862\n",
      "Episode 895000, Loss: -7.1446, Reward: 0.3921, Avg Reward: 0.2873\n",
      "Episode 896000, Loss: -13.0493, Reward: 0.4757, Avg Reward: 0.2727\n",
      "Episode 897000, Loss: 0.4211, Reward: 0.2022, Avg Reward: 0.2710\n",
      "Episode 898000, Loss: -0.0001, Reward: 0.1102, Avg Reward: 0.2868\n",
      "Episode 899000, Loss: -3.6735, Reward: 0.3494, Avg Reward: 0.2840\n",
      "Episode 900000, Loss: -0.0001, Reward: 0.0516, Avg Reward: 0.2846\n",
      "Episode 901000, Loss: -3.5813, Reward: 0.3538, Avg Reward: 0.2898\n",
      "Episode 902000, Loss: -0.0000, Reward: 0.1870, Avg Reward: 0.2789\n",
      "Episode 903000, Loss: 2.3517, Reward: 0.2234, Avg Reward: 0.2873\n",
      "Episode 904000, Loss: 0.0000, Reward: 0.1902, Avg Reward: 0.2867\n",
      "Episode 905000, Loss: 0.0000, Reward: 0.0716, Avg Reward: 0.2867\n",
      "Episode 906000, Loss: 2.3602, Reward: 0.1658, Avg Reward: 0.2805\n",
      "Episode 907000, Loss: 1.5174, Reward: 0.2515, Avg Reward: 0.2760\n",
      "Episode 908000, Loss: -0.0001, Reward: 0.1755, Avg Reward: 0.2737\n",
      "Episode 909000, Loss: -12.3593, Reward: 0.4631, Avg Reward: 0.2909\n",
      "Episode 910000, Loss: -13.2783, Reward: 0.4850, Avg Reward: 0.2812\n",
      "Episode 911000, Loss: 0.0000, Reward: 0.1999, Avg Reward: 0.2756\n",
      "Episode 912000, Loss: -40.8211, Reward: 0.8805, Avg Reward: 0.2716\n",
      "Episode 913000, Loss: -17.6955, Reward: 0.5270, Avg Reward: 0.2716\n",
      "Episode 914000, Loss: 5.4297, Reward: 0.2077, Avg Reward: 0.2900\n",
      "Episode 915000, Loss: -0.3029, Reward: 0.3356, Avg Reward: 0.2899\n",
      "Episode 916000, Loss: -16.3554, Reward: 0.5179, Avg Reward: 0.2819\n",
      "Episode 917000, Loss: -9.1506, Reward: 0.4213, Avg Reward: 0.2803\n",
      "Episode 918000, Loss: 0.0000, Reward: 0.0259, Avg Reward: 0.2951\n",
      "Episode 919000, Loss: -2.8190, Reward: 0.3924, Avg Reward: 0.2801\n",
      "Episode 920000, Loss: 3.7357, Reward: 0.0453, Avg Reward: 0.2927\n",
      "Episode 921000, Loss: 0.0000, Reward: 0.2086, Avg Reward: 0.2898\n",
      "Episode 922000, Loss: -11.0996, Reward: 0.4352, Avg Reward: 0.2870\n",
      "Episode 923000, Loss: 4.4389, Reward: 0.1683, Avg Reward: 0.2790\n",
      "Episode 924000, Loss: -0.0000, Reward: 0.2907, Avg Reward: 0.2863\n",
      "Episode 925000, Loss: -5.8912, Reward: 0.3721, Avg Reward: 0.2866\n",
      "Episode 926000, Loss: -0.0000, Reward: 0.3034, Avg Reward: 0.2817\n",
      "Episode 927000, Loss: -0.0000, Reward: 0.3546, Avg Reward: 0.2866\n",
      "Episode 928000, Loss: 0.0000, Reward: 0.2630, Avg Reward: 0.2702\n",
      "Episode 929000, Loss: -5.5397, Reward: 0.3398, Avg Reward: 0.2773\n",
      "Episode 930000, Loss: 0.0000, Reward: 0.0010, Avg Reward: 0.2793\n",
      "Episode 931000, Loss: 1.0049, Reward: 0.2419, Avg Reward: 0.2866\n",
      "Episode 932000, Loss: -0.0138, Reward: 0.1183, Avg Reward: 0.2817\n",
      "Episode 933000, Loss: 3.7325, Reward: 0.2046, Avg Reward: 0.2871\n",
      "Episode 934000, Loss: 0.0000, Reward: 0.0872, Avg Reward: 0.2774\n",
      "Episode 935000, Loss: -0.0000, Reward: 0.4769, Avg Reward: 0.2824\n",
      "Episode 936000, Loss: -14.9257, Reward: 0.4647, Avg Reward: 0.2725\n",
      "Episode 937000, Loss: 5.1216, Reward: 0.2100, Avg Reward: 0.2828\n",
      "Episode 938000, Loss: 0.0000, Reward: 0.1105, Avg Reward: 0.2677\n",
      "Episode 939000, Loss: 0.0000, Reward: 0.0464, Avg Reward: 0.2767\n",
      "Episode 940000, Loss: -0.0000, Reward: 0.3264, Avg Reward: 0.2741\n",
      "Episode 941000, Loss: -28.6418, Reward: 0.7092, Avg Reward: 0.2726\n",
      "Episode 942000, Loss: 0.0000, Reward: 0.2811, Avg Reward: 0.2823\n",
      "Episode 943000, Loss: 0.1105, Reward: 0.2785, Avg Reward: 0.2836\n",
      "Episode 944000, Loss: -41.5178, Reward: 0.9276, Avg Reward: 0.2824\n",
      "Episode 945000, Loss: -0.5170, Reward: 0.3001, Avg Reward: 0.2915\n",
      "Episode 946000, Loss: -0.0000, Reward: 0.4611, Avg Reward: 0.2875\n",
      "Episode 947000, Loss: -19.0520, Reward: 0.5539, Avg Reward: 0.2805\n",
      "Episode 948000, Loss: -4.3967, Reward: 0.3547, Avg Reward: 0.2885\n",
      "Episode 949000, Loss: 0.0189, Reward: 0.3845, Avg Reward: 0.2828\n",
      "Episode 950000, Loss: 0.0000, Reward: 0.0748, Avg Reward: 0.2719\n",
      "Episode 951000, Loss: 0.0000, Reward: 0.0349, Avg Reward: 0.2798\n",
      "Episode 952000, Loss: -0.0005, Reward: 0.0805, Avg Reward: 0.2894\n",
      "Episode 953000, Loss: -19.2686, Reward: 0.5737, Avg Reward: 0.2822\n",
      "Episode 954000, Loss: -0.0000, Reward: 0.1309, Avg Reward: 0.2806\n",
      "Episode 955000, Loss: -20.0841, Reward: 0.6244, Avg Reward: 0.2694\n",
      "Episode 956000, Loss: 8.3242, Reward: 0.0686, Avg Reward: 0.2734\n",
      "Episode 957000, Loss: -10.9378, Reward: 0.4143, Avg Reward: 0.2786\n",
      "Episode 958000, Loss: -19.5411, Reward: 0.5619, Avg Reward: 0.2778\n",
      "Episode 959000, Loss: 0.0000, Reward: 0.1273, Avg Reward: 0.2863\n",
      "Episode 960000, Loss: 3.7332, Reward: 0.1853, Avg Reward: 0.2884\n",
      "Episode 961000, Loss: 0.0000, Reward: 0.2038, Avg Reward: 0.2913\n",
      "Episode 962000, Loss: -7.3774, Reward: 0.3885, Avg Reward: 0.2819\n",
      "Episode 963000, Loss: -0.0001, Reward: 0.2459, Avg Reward: 0.2675\n",
      "Episode 964000, Loss: 0.0000, Reward: 0.1548, Avg Reward: 0.2774\n",
      "Episode 965000, Loss: -10.9497, Reward: 0.4283, Avg Reward: 0.2658\n",
      "Episode 966000, Loss: 0.0000, Reward: 0.0254, Avg Reward: 0.2779\n",
      "Episode 967000, Loss: -7.1203, Reward: 0.4670, Avg Reward: 0.2721\n",
      "Episode 968000, Loss: 4.9994, Reward: 0.2052, Avg Reward: 0.2777\n",
      "Episode 969000, Loss: -10.1405, Reward: 0.4664, Avg Reward: 0.2789\n",
      "Episode 970000, Loss: 0.0000, Reward: 0.1431, Avg Reward: 0.2763\n",
      "Episode 971000, Loss: -0.0000, Reward: 0.3606, Avg Reward: 0.2812\n",
      "Episode 972000, Loss: 6.0100, Reward: 0.0171, Avg Reward: 0.2823\n",
      "Episode 973000, Loss: -0.4753, Reward: 0.2663, Avg Reward: 0.2657\n",
      "Episode 974000, Loss: -1.6888, Reward: 0.2901, Avg Reward: 0.2558\n",
      "Episode 975000, Loss: -0.0000, Reward: 0.2551, Avg Reward: 0.2690\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (4,)) of distribution Categorical(logits: torch.Size([4])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([nan, nan, nan, nan], grad_fn=<SubBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 306\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Main Execution\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# Train the LSTM-based policy network using the RL approach.\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     policy_net \u001b[38;5;241m=\u001b[39m train_policy(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# Evaluate on a test set.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     test_policy(policy_net, num_tests\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 238\u001b[0m, in \u001b[0;36mtrain_policy\u001b[0;34m(num_episodes, lr, print_every)\u001b[0m\n\u001b[1;32m    235\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m target_unitary_to_tensor(U_target)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape: (1, input_dim)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Forward pass: sample a candidate pulse sequence.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m actions_batch, log_prob \u001b[38;5;241m=\u001b[39m policy_net(target_tensor)\n\u001b[1;32m    239\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions_batch[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assume batch_size=1.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Build candidate sequence from actions.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 177\u001b[0m, in \u001b[0;36mPolicyNetworkLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m fraction_raw \u001b[38;5;241m=\u001b[39m step_out[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_couplings\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_couplings\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# For Beta distribution parameters.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Sample from the coupling distribution.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m coupling_dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mcoupling_logits)\n\u001b[1;32m    178\u001b[0m coupling_index \u001b[38;5;241m=\u001b[39m coupling_dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    179\u001b[0m log_prob_sum \u001b[38;5;241m=\u001b[39m log_prob_sum \u001b[38;5;241m+\u001b[39m coupling_dist\u001b[38;5;241m.\u001b[39mlog_prob(coupling_index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/distributions/categorical.py:72\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/distributions/distribution.py:71\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 71\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (4,)) of distribution Categorical(logits: torch.Size([4])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([nan, nan, nan, nan], grad_fn=<SubBackward0>)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Beta\n",
    "from scipy.linalg import expm\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions (as provided)\n",
    "# ---------------------------\n",
    "\n",
    "def coupling_operator_with_phase(i, j, dim, phi):\n",
    "    op = np.zeros((dim, dim), dtype=complex)\n",
    "    op[i, j] = np.exp(+1j * phi)\n",
    "    op[j, i] = np.exp(-1j * phi)\n",
    "    return op\n",
    "\n",
    "def pulse_duration_for_fraction(f, Omega):\n",
    "    theta = np.pi * np.array(f)\n",
    "    return theta / Omega if Omega != 0 else 0.0\n",
    "\n",
    "def unitary(couplings, rabi_freqs, fractions, fixed_phase_flags, dim):\n",
    "    U_seq = np.eye(dim, dtype=complex)\n",
    "    for (levels, Omega, frac, fix_pflag) in zip(couplings, rabi_freqs, fractions, fixed_phase_flags):\n",
    "        i, j = levels\n",
    "        phi_fixed = fix_pflag * np.pi\n",
    "        total_phase = phi_fixed\n",
    "        H_op = coupling_operator_with_phase(i, j, dim, total_phase)\n",
    "        H_coupling = 0.5 * Omega * H_op\n",
    "        t_pulse = pulse_duration_for_fraction(frac, Omega)\n",
    "        U_pulse = expm(-1j * H_coupling * t_pulse)\n",
    "        U_seq = U_pulse @ U_seq\n",
    "    return U_seq\n",
    "\n",
    "def fix_couplings_and_phases(couplings, fixed_phase_flags):\n",
    "    new_couplings = []\n",
    "    new_fixed_phase_flags = []\n",
    "    for (cpl, phase_flag) in zip(couplings, fixed_phase_flags):\n",
    "        i, j = cpl\n",
    "        if i != 0 and j == 0:\n",
    "            cpl_fixed = (0, i)\n",
    "            phase_flag_fixed = phase_flag + 1.0\n",
    "        else:\n",
    "            cpl_fixed = cpl\n",
    "            phase_flag_fixed = phase_flag\n",
    "        new_couplings.append(cpl_fixed)\n",
    "        new_fixed_phase_flags.append(phase_flag_fixed)\n",
    "    return new_couplings, new_fixed_phase_flags\n",
    "\n",
    "def compute_fidelity(U_target, U_pred, dim):\n",
    "    # Fidelity measure using the normalized absolute trace overlap.\n",
    "    fid = np.abs(np.trace(np.conjugate(U_target.T) @ U_pred)) / dim\n",
    "    return fid\n",
    "\n",
    "# ---------------------------\n",
    "# Problem Setup Parameters\n",
    "# ---------------------------\n",
    "\n",
    "dim = 5                  # Hilbert space dimension\n",
    "sequence_length = 7      # Number of pulses in the sequence\n",
    "max_fraction = 2       # Maximum value for the 'fraction' parameter\n",
    "# For the RL agent, we fix rabi frequencies to 1 for every pulse.\n",
    "rabi_freqs = [1] * sequence_length\n",
    "\n",
    "# Define a list of allowed couplings (discrete choices) for the policy.\n",
    "allowed_couplings = [(0, 1), (0, 2), (0, 3), (0, 4)]\n",
    "num_allowed_couplings = len(allowed_couplings)\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Generation Functions\n",
    "# ---------------------------\n",
    "def generate_random_sequence():\n",
    "    \"\"\"Generates a random pulse sequence using uniform sampling.\n",
    "       Returns:\n",
    "          couplings: list of tuples (each chosen from allowed_couplings)\n",
    "          fractions: list of continuous parameters in [0, max_fraction]\n",
    "          fixed_phase_flags: list of binary values (0 or 1)\n",
    "    \"\"\"\n",
    "    couplings = []\n",
    "    fractions = []\n",
    "    fixed_phase_flags = []\n",
    "    for _ in range(sequence_length):\n",
    "        # Randomly choose a coupling from the allowed list.\n",
    "        idx = np.random.randint(0, num_allowed_couplings)\n",
    "        couplings.append(allowed_couplings[idx])\n",
    "        # Sample a random fraction uniformly.\n",
    "        frac = np.random.uniform(0.0, max_fraction)\n",
    "        fractions.append(frac)\n",
    "        # Sample a binary fixed-phase flag.\n",
    "        fix_flag = np.random.randint(0, 2)\n",
    "        fixed_phase_flags.append(fix_flag)\n",
    "        \n",
    "    # Apply the fix function (if applicable)\n",
    "    couplings, fixed_phase_flags = fix_couplings_and_phases(couplings, fixed_phase_flags)\n",
    "    return couplings, fractions, fixed_phase_flags\n",
    "\n",
    "def generate_random_target_unitary():\n",
    "    \"\"\"Generates a target unitary by sampling a random pulse sequence.\n",
    "       Returns:\n",
    "          U_target (np.ndarray): the resulting unitary matrix.\n",
    "          parameters: the pulse parameters used (for reference).\n",
    "    \"\"\"\n",
    "    couplings, fractions, fixed_phase_flags = generate_random_sequence()\n",
    "    U_target = unitary(couplings, rabi_freqs, fractions, fixed_phase_flags, dim)\n",
    "    return U_target, (couplings, fractions, fixed_phase_flags)\n",
    "\n",
    "# ---------------------------\n",
    "# LSTM-Based Policy Network Definition\n",
    "# ---------------------------\n",
    "class PolicyNetworkLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sequence_length, num_couplings):\n",
    "        \"\"\"\n",
    "        input_dim: Dimension of the flattened target unitary (real and imaginary parts concatenated)\n",
    "        hidden_dim: Hidden layer size for both the encoder and LSTM\n",
    "        sequence_length: Number of pulses (time steps)\n",
    "        num_couplings: Number of allowed coupling choices.\n",
    "        \"\"\"\n",
    "        super(PolicyNetworkLSTM, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_couplings = num_couplings\n",
    "        # Each time step will output:\n",
    "        # - Coupling logits (num_couplings)\n",
    "        # - Fixed-phase logits (2)\n",
    "        # - Fraction parameters (2 for Beta: raw_alpha, raw_beta)\n",
    "        self.output_per_step = num_couplings + 2 + 2\n",
    "\n",
    "        # Encoder: Encode the target unitary into a hidden representation.\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # LSTM decoder: receives a learned start token at each time step.\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        # Learned start token; shape: (1, hidden_dim) which will be repeated.\n",
    "        self.start_token = nn.Parameter(torch.zeros(1, hidden_dim))\n",
    "        \n",
    "        # Decoder to produce output for each time step from the LSTM hidden state.\n",
    "        self.decoder = nn.Linear(hidden_dim, self.output_per_step)\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, input_dim) representing the target unitary.\n",
    "        Returns for each sample in the batch: a list of actions per time step and the sum of their log probabilities.\n",
    "        Each action is a dictionary with keys: 'coupling', 'fraction', and 'fixed_phase_flag'.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        # Encode the target unitary.\n",
    "        encoded = self.relu(self.encoder(x))  # shape: (batch_size, hidden_dim)\n",
    "        # Use the encoded representation as the initial hidden state.\n",
    "        h0 = encoded.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
    "        c0 = torch.zeros_like(h0)  # initialize cell state to zeros\n",
    "        \n",
    "        # Prepare LSTM input: we use the same start token at each time step.\n",
    "        # Create tokens with shape: (batch_size, sequence_length, hidden_dim)\n",
    "        tokens = self.start_token.expand(batch_size, self.sequence_length, -1)\n",
    "        \n",
    "        # Generate the sequence from the LSTM.\n",
    "        lstm_out, _ = self.lstm(tokens, (h0, c0))  # shape: (batch_size, sequence_length, hidden_dim)\n",
    "        # Project LSTM outputs to the desired output shape.\n",
    "        decoded = self.decoder(lstm_out)  # shape: (batch_size, sequence_length, output_per_step)\n",
    "        \n",
    "        actions_batch = []  # List to hold actions per sample in the batch.\n",
    "        log_probs = []      # List for the sum of log probabilities for each sample.\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            actions = []\n",
    "            log_prob_sum = 0\n",
    "            # Process each time step.\n",
    "            for t in range(self.sequence_length):\n",
    "                step_out = decoded[b, t]\n",
    "                # Split outputs into parts.\n",
    "                coupling_logits = step_out[0:self.num_couplings]  # Discrete coupling selection.\n",
    "                fixed_logits = step_out[self.num_couplings:self.num_couplings+2]  # Fixed-phase binary decision.\n",
    "                fraction_raw = step_out[self.num_couplings+2:self.num_couplings+4]  # For Beta distribution parameters.\n",
    "                \n",
    "                # Sample from the coupling distribution.\n",
    "                coupling_dist = Categorical(logits=coupling_logits)\n",
    "                coupling_index = coupling_dist.sample()\n",
    "                log_prob_sum = log_prob_sum + coupling_dist.log_prob(coupling_index)\n",
    "                \n",
    "                # Sample from the fixed-phase distribution.\n",
    "                fixed_dist = Categorical(logits=fixed_logits)\n",
    "                fixed_phase_flag = fixed_dist.sample()\n",
    "                log_prob_sum = log_prob_sum + fixed_dist.log_prob(fixed_phase_flag)\n",
    "                \n",
    "                # Sample the fraction from a Beta distribution.\n",
    "                alpha = self.softplus(fraction_raw[0]) + 1.0\n",
    "                beta_param = self.softplus(fraction_raw[1]) + 1.0\n",
    "                fraction_dist = Beta(alpha, beta_param)\n",
    "                fraction_sample = fraction_dist.rsample()  # Use rsample for reparameterization.\n",
    "                # Scale to the desired range [0, max_fraction].\n",
    "                fraction = fraction_sample * max_fraction\n",
    "                log_prob_sum = log_prob_sum + fraction_dist.log_prob(fraction_sample)\n",
    "                \n",
    "                # Append the action.\n",
    "                actions.append({\n",
    "                    'coupling': int(coupling_index.item()),\n",
    "                    'fraction': fraction.item(),\n",
    "                    'fixed_phase_flag': int(fixed_phase_flag.item())\n",
    "                })\n",
    "            actions_batch.append(actions)\n",
    "            log_probs.append(log_prob_sum)\n",
    "        \n",
    "        return actions_batch, torch.stack(log_probs)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: Convert Target Unitary to Network Input\n",
    "# ---------------------------\n",
    "def target_unitary_to_tensor(U):\n",
    "    \"\"\"\n",
    "    Convert U (a complex matrix) into a real tensor that concatenates the flattened real and imaginary parts.\n",
    "    \"\"\"\n",
    "    U_real = np.real(U).flatten()\n",
    "    U_imag = np.imag(U).flatten()\n",
    "    U_concat = np.concatenate([U_real, U_imag])\n",
    "    return torch.tensor(U_concat, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop (REINFORCE)\n",
    "# ---------------------------\n",
    "def train_policy(num_episodes=1000, lr=1e-3, print_every=100):\n",
    "    # Input dimension: dim*dim*2 (for real and imaginary parts).\n",
    "    input_dim = dim * dim * 2\n",
    "    hidden_dim = 128\n",
    "    policy_net = PolicyNetworkLSTM(input_dim, hidden_dim, sequence_length, num_allowed_couplings)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    # Running baseline for variance reduction.\n",
    "    baseline = None\n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Generate a target unitary.\n",
    "        U_target, true_params = generate_random_target_unitary()\n",
    "        target_tensor = target_unitary_to_tensor(U_target).unsqueeze(0)  # shape: (1, input_dim)\n",
    "        \n",
    "        # Forward pass: sample a candidate pulse sequence.\n",
    "        actions_batch, log_prob = policy_net(target_tensor)\n",
    "        actions = actions_batch[0]  # Assume batch_size=1.\n",
    "        \n",
    "        # Build candidate sequence from actions.\n",
    "        candidate_couplings = []\n",
    "        candidate_fractions = []\n",
    "        candidate_fixed_flags = []\n",
    "        for action in actions:\n",
    "            candidate_couplings.append(allowed_couplings[action['coupling']])\n",
    "            candidate_fractions.append(action['fraction'])\n",
    "            candidate_fixed_flags.append(action['fixed_phase_flag'])\n",
    "        \n",
    "        # Optionally apply the fix on couplings/phases.\n",
    "        candidate_couplings, candidate_fixed_flags = fix_couplings_and_phases(candidate_couplings, candidate_fixed_flags)\n",
    "        U_pred = unitary(candidate_couplings, rabi_freqs, candidate_fractions, candidate_fixed_flags, dim)\n",
    "        \n",
    "        # Compute reward based on fidelity.\n",
    "        reward = compute_fidelity(U_target, U_pred, dim)\n",
    "        reward_history.append(reward)\n",
    "        \n",
    "        # Update running baseline.\n",
    "        if baseline is None:\n",
    "            baseline = reward\n",
    "        else:\n",
    "            baseline = 0.99 * baseline + 0.01 * reward\n",
    "        \n",
    "        # REINFORCE loss: maximize reward by minimizing -log_prob * (reward - baseline)\n",
    "        loss = -log_prob * (reward - baseline)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(reward_history[-print_every:])\n",
    "            print(f\"Episode {episode+1}, Loss: {loss.item():.4f}, Reward: {reward:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
    "    \n",
    "    return policy_net\n",
    "\n",
    "# ---------------------------\n",
    "# Testing the Trained Policy\n",
    "# ---------------------------\n",
    "def test_policy(policy_net, num_tests=10):\n",
    "    test_rewards = []\n",
    "    for i in range(num_tests):\n",
    "        U_target, true_params = generate_random_target_unitary()\n",
    "        target_tensor = target_unitary_to_tensor(U_target).unsqueeze(0)\n",
    "        actions_batch, _ = policy_net(target_tensor)\n",
    "        actions = actions_batch[0]\n",
    "        candidate_couplings = []\n",
    "        candidate_fractions = []\n",
    "        candidate_fixed_flags = []\n",
    "        for action in actions:\n",
    "            candidate_couplings.append(allowed_couplings[action['coupling']])\n",
    "            candidate_fractions.append(action['fraction'])\n",
    "            candidate_fixed_flags.append(action['fixed_phase_flag'])\n",
    "        candidate_couplings, candidate_fixed_flags = fix_couplings_and_phases(candidate_couplings, candidate_fixed_flags)\n",
    "        U_pred = unitary(candidate_couplings, rabi_freqs, candidate_fractions, candidate_fixed_flags, dim)\n",
    "        reward = compute_fidelity(U_target, U_pred, dim)\n",
    "        test_rewards.append(reward)\n",
    "        print(f\"Test {i+1}: Fidelity = {reward:.4f}\")\n",
    "    print(f\"Average test fidelity: {np.mean(test_rewards):.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Train the LSTM-based policy network using the RL approach.\n",
    "    policy_net = train_policy(num_episodes=1000000, lr=1e-2, print_every=1000)\n",
    "    # Evaluate on a test set.\n",
    "    test_policy(policy_net, num_tests=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8a6c4-2247-43f8-a8fa-24afbf7c210e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
